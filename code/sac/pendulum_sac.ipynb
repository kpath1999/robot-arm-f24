{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fRTAFe-S8iCo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions.normal import Normal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# buffer.py\n",
        "class ReplayBuffer():\n",
        "  def __init__(self, max_size, input_shape, n_actions):\n",
        "    self.mem_size = max_size\n",
        "    self.mem_cntr = 0\n",
        "    self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
        "    self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
        "    self.action_memory = np.zeros((self.mem_size, n_actions))\n",
        "    self.reward_memory = np.zeros(self.mem_size)\n",
        "    self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool_)\n",
        "\n",
        "  def store_transition(self, state, action, reward, state_, done):\n",
        "    index = self.mem_cntr % self.mem_size\n",
        "    self.new_state_memory[index] = state_\n",
        "    self.state_memory[index] = state\n",
        "    self.action_memory[index] = action\n",
        "    self.reward_memory[index] = reward\n",
        "    self.terminal_memory[index] = done\n",
        "\n",
        "    self.mem_cntr += 1\n",
        "\n",
        "  def sample_buffer(self, batch_size):\n",
        "    max_mem = min(self.mem_cntr, self.mem_size)\n",
        "    batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "    states = self.state_memory[batch]\n",
        "    states_ = self.new_state_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    dones = self.terminal_memory[batch]\n",
        "\n",
        "    return states, actions, rewards, states_, dones"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3k7oYjD8n-Y",
        "outputId": "9e0c461c-4b64-45d5-883f-8deeb2e8f46a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# networks.py\n",
        "class CriticNetwork(nn.Module):\n",
        "  def __init__(self, beta, input_dims, n_actions, fc1_dims=256, fc2_dims=256, name='critic', chkpt_dir='tmp/sac'):\n",
        "    super(CriticNetwork, self).__init__()\n",
        "    self.input_dims = input_dims\n",
        "    self.fc1_dims = fc1_dims\n",
        "    self.fc2_dims = fc2_dims\n",
        "    self.n_actions = n_actions\n",
        "    self.name = name\n",
        "    self.checkpoint_dir = chkpt_dir\n",
        "    self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_sac')\n",
        "\n",
        "    self.fc1 = nn.Linear(self.input_dims[0] + n_actions, self.fc1_dims)   # incorporate state and action pairs\n",
        "    self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "    self.q = nn.Linear(self.fc2_dims, 1)\n",
        "\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
        "    self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "    self.to(self.device)\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    action_value = self.fc1(T.cat([state, action], dim=1))\n",
        "    action_value = F.relu(action_value)\n",
        "    action_value = self.fc2(action_value)\n",
        "    action_value = F.relu(action_value)\n",
        "    q = self.q(action_value)\n",
        "    return q\n",
        "\n",
        "  def save_checkpoint(self):\n",
        "    T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "  def load_checkpoint(self):\n",
        "    self.load_state_dict(T.load(self.checkpoint_file))"
      ],
      "metadata": {
        "id": "syK5g5Zs-FuX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# networks.py\n",
        "class ValueNetwork(nn.Module):\n",
        "  def __init__(self, beta, input_dims, fc1_dims=256, fc2_dims=256, name='value', chkpt_dir='tmp/sac'):\n",
        "    super(ValueNetwork, self).__init__()\n",
        "    self.input_dims = input_dims\n",
        "    self.fc1_dims = fc1_dims\n",
        "    self.fc2_dims = fc2_dims\n",
        "    self.name = name\n",
        "    self.checkpoint_dir = chkpt_dir\n",
        "    self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_sac')\n",
        "\n",
        "    self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
        "    self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "    self.v = nn.Linear(self.fc2_dims, 1)\n",
        "\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
        "    self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "    self.to(self.device)\n",
        "\n",
        "  def forward(self, state):\n",
        "    state_value = self.fc1(state)\n",
        "    state_value = F.relu(state_value)\n",
        "    state_value = self.fc2(state_value)\n",
        "    state_value = F.relu(state_value)\n",
        "\n",
        "    v = self.v(state_value)\n",
        "    return v\n",
        "\n",
        "  def save_checkpoint(self):\n",
        "    T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "  def load_checkpoint(self):\n",
        "    self.load_state_dict(T.load(self.checkpoint_file))"
      ],
      "metadata": {
        "id": "wzAT5mfqAOmc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# networks.py\n",
        "# the harder part of the problem\n",
        "class ActorNetwork(nn.Module):\n",
        "  def __init__(self, alpha, input_dims, max_action, fc1_dims=256, fc2_dims=256,\n",
        "               n_actions=2, name='actor', chkpt_dir='tmp/sac'):\n",
        "    super(ActorNetwork, self).__init__()\n",
        "    self.input_dims = input_dims\n",
        "    self.fc1_dims = fc1_dims\n",
        "    self.fc2_dims = fc2_dims\n",
        "    self.n_actions = n_actions\n",
        "    self.name = name\n",
        "    self.checkpoint_dir = chkpt_dir\n",
        "    self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_sac')\n",
        "    self.max_action = max_action\n",
        "    self.reparam_noise = 1e-6\n",
        "\n",
        "    self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
        "    self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "    self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n",
        "    self.sigma = nn.Linear(self.fc2_dims, self.n_actions)\n",
        "\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "    self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "\n",
        "    self.to(self.device)\n",
        "\n",
        "  def forward(self, state):\n",
        "    prob = self.fc1(state)\n",
        "    prob = F.relu(prob)\n",
        "    prob = self.fc2(prob)\n",
        "    prob = F.relu(prob)\n",
        "\n",
        "    mu = self.mu(prob)\n",
        "    sigma = self.sigma(prob)\n",
        "\n",
        "    sigma = T.clamp(sigma, min=self.reparam_noise, max=1)\n",
        "\n",
        "    return mu, sigma\n",
        "\n",
        "  def sample_normal(self, state, reparameterize=True):\n",
        "    mu, sigma = self.forward(state)\n",
        "    probabilities = Normal(mu, sigma)\n",
        "\n",
        "    if reparameterize:\n",
        "      actions = probabilities.rsample()\n",
        "    else:\n",
        "      actions = probabilities.sample()\n",
        "\n",
        "    action = T.tanh(actions) * T.tensor(self.max_action).to(self.device)\n",
        "    log_probs = probabilities.log_prob(actions)\n",
        "    log_probs -= T.log(1 - action.pow(2) + self.reparam_noise)\n",
        "    log_probs = log_probs.sum(1, keepdim=True)\n",
        "\n",
        "    return action, log_probs\n",
        "\n",
        "  def save_checkpoint(self):\n",
        "    T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "  def load_checkpoint(self):\n",
        "    self.load_state_dict(T.load(self.checkpoint_file))"
      ],
      "metadata": {
        "id": "p90SK_19Bvgh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sac_torch.py\n",
        "class Agent():\n",
        "  def __init__(self, alpha=0.0003, beta=0.0003, input_dims=[8], env=None,\n",
        "               gamma=0.99, n_actions=2, max_size=1000000, tau=0.005, layer1_size=256,\n",
        "               layer2_size=256, batch_size=256, reward_scale=2, chkpt_dir='tmp/sac'):\n",
        "    self.gamma = gamma\n",
        "    self.tau = tau\n",
        "    self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
        "    self.batch_size = batch_size\n",
        "    self.n_actions = n_actions\n",
        "\n",
        "    self.actor = ActorNetwork(alpha, input_dims, n_actions=n_actions, name='actor',\n",
        "                              max_action=env.action_space.high, chkpt_dir=chkpt_dir)\n",
        "    self.critic_1 = CriticNetwork(beta, input_dims, n_actions=n_actions, name='critic_1')\n",
        "    self.critic_2 = CriticNetwork(beta, input_dims, n_actions=n_actions, name='critic_2')\n",
        "    self.value = ValueNetwork(beta, input_dims, name='value')\n",
        "    self.target_value = ValueNetwork(beta, input_dims, name='target_value')\n",
        "\n",
        "    self.scale = reward_scale\n",
        "\n",
        "    self.update_network_parameters(tau=1)\n",
        "\n",
        "  def choose_action(self, observation):\n",
        "    state = T.Tensor([observation]).to(self.actor.device)\n",
        "    actions, _ = self.actor.sample_normal(state, reparameterize=False)\n",
        "    return actions.cpu().detach().numpy()[0]\n",
        "\n",
        "  def remember(self, state, action, reward, new_state, done):\n",
        "    self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "  def update_network_parameters(self, tau=None):\n",
        "    if tau is None:\n",
        "      tau = self.tau\n",
        "    target_value_params = self.target_value.named_parameters()\n",
        "    value_params = self.value.named_parameters()\n",
        "\n",
        "    target_value_state_dict = dict(target_value_params)\n",
        "    value_state_dict = dict(value_params)\n",
        "\n",
        "    for name in value_state_dict:\n",
        "      value_state_dict[name] = tau * value_state_dict[name].clone() + \\\n",
        "                                  (1 - tau) * target_value_state_dict[name].clone()\n",
        "\n",
        "    self.target_value.load_state_dict(value_state_dict)\n",
        "\n",
        "  def save_models(self):\n",
        "    print('...saving models...')\n",
        "    self.actor.save_checkpoint()\n",
        "    self.value.save_checkpoint()\n",
        "    self.target_value.save_checkpoint()\n",
        "    self.critic_1.save_checkpoint()\n",
        "    self.critic_2.save_checkpoint()\n",
        "\n",
        "  def load_models(self):\n",
        "    print('...loading models...')\n",
        "    self.actor.load_checkpoint()\n",
        "    self.value.load_checkpoint()\n",
        "    self.target_value.load_checkpoint()\n",
        "    self.critic_1.load_checkpoint()\n",
        "    self.critic_2.load_checkpoint()\n",
        "\n",
        "  def learn(self):\n",
        "    if self.memory.mem_cntr < self.batch_size:\n",
        "      return\n",
        "\n",
        "    state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "    reward = T.tensor(reward, dtype=T.float).to(self.actor.device)\n",
        "    state = T.tensor(state, dtype=T.float).to(self.actor.device)\n",
        "    state_ = T.tensor(new_state, dtype=T.float).to(self.actor.device)\n",
        "    done = T.tensor(done).to(self.actor.device)\n",
        "    action = T.tensor(action, dtype=T.float).to(self.actor.device)\n",
        "\n",
        "    value = self.value(state).view(-1)\n",
        "    value_ = self.target_value(state_).view(-1)\n",
        "    value_[done] = 0.0\n",
        "\n",
        "    actions, log_probs = self.actor.sample_normal(state, reparameterize=False)\n",
        "    log_probs = log_probs.view(-1)\n",
        "    q1_new_policy = self.critic_1.forward(state, actions)\n",
        "    q2_new_policy = self.critic_2.forward(state, actions)\n",
        "    critic_value = T.min(q1_new_policy, q2_new_policy)\n",
        "    critic_value = critic_value.view(-1)\n",
        "\n",
        "    self.value.optimizer.zero_grad()\n",
        "    value_target = critic_value - log_probs\n",
        "    value_loss = 0.5 * F.mse_loss(value, value_target)\n",
        "    value_loss.backward(retain_graph=True)\n",
        "    self.value.optimizer.step()\n",
        "\n",
        "    actions, log_probs = self.actor.sample_normal(state, reparameterize=True)\n",
        "    log_probs = log_probs.view(-1)\n",
        "    q1_new_policy = self.critic_1.forward(state, actions)\n",
        "    q2_new_policy = self.critic_2.forward(state, actions)\n",
        "\n",
        "    actor_loss = log_probs - critic_value\n",
        "    actor_loss = T.mean(actor_loss)\n",
        "    self.actor.optimizer.zero_grad()\n",
        "    actor_loss.backward(retain_graph=True)\n",
        "    self.actor.optimizer.step()\n",
        "\n",
        "    self.critic_1.optimizer.zero_grad()\n",
        "    self.critic_2.optimizer.zero_grad()\n",
        "    q_hat = self.scale * reward + self.gamma * value_\n",
        "    q1_old_policy = self.critic_1.forward(state, action).view(-1)\n",
        "    q2_old_policy = self.critic_2.forward(state, action).view(-1)\n",
        "    critic_1_loss = 0.5 * F.mse_loss(q1_old_policy, q_hat)\n",
        "    critic_2_loss = 0.5 * F.mse_loss(q2_old_policy, q_hat)\n",
        "\n",
        "    critic_loss = critic_1_loss + critic_2_loss\n",
        "    critic_loss.backward()\n",
        "    self.critic_1.optimizer.step()\n",
        "    self.critic_2.optimizer.step()\n",
        "\n",
        "    self.update_network_parameters()"
      ],
      "metadata": {
        "id": "YcXgDrwEE8JW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybullet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8HimOThKMmm",
        "outputId": "a58f4932-8450-4051-9082-962e5401f619"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pybullet_envs\n",
        "import gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1erYCFzKRPG",
        "outputId": "08c0f50d-89e9-40f3-e547-1b393aef21f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcfeVi4XK29d",
        "outputId": "1b0745b4-c882-44c8-db82-f9f470c25ad3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  env = gym.make('InvertedPendulumBulletEnv-v0')\n",
        "  agent = Agent(input_dims=env.observation_space.shape, env=env,\n",
        "               n_actions=env.action_space.shape[0])\n",
        "  n_games = 250\n",
        "  filename = 'inverted_pendulum.png'\n",
        "\n",
        "  figure_file = 'plots/' + filename\n",
        "\n",
        "  best_score = env.reward_range[0]\n",
        "  score_history = []\n",
        "  load_checkpoint = False\n",
        "\n",
        "  if load_checkpoint:\n",
        "    agent.load_models()\n",
        "    env.render(mode='human')\n",
        "\n",
        "  for i in range(n_games):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "      action = agent.choose_action(observation)\n",
        "      observation_, reward, done, info = env.step(action)\n",
        "      score += reward\n",
        "      agent.remember(observation, action, reward, observation_, done)\n",
        "      if not load_checkpoint:\n",
        "        agent.learn()\n",
        "      observation = observation_\n",
        "    score_history.append(score)\n",
        "    avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "    if avg_score > best_score:\n",
        "      best_score = avg_score\n",
        "      #if not load_checkpoint:\n",
        "        #agent.save_models()\n",
        "\n",
        "    print('episode ', i, 'score %.1f' % score, 'avg score %.1f' % avg_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY3zmbvgKj-V",
        "outputId": "bbe8674a-2ace-49e9-8cc8-751fa181da11"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode  0 score 22.0 avg score 22.0\n",
            "episode  1 score 25.0 avg score 23.5\n",
            "episode  2 score 33.0 avg score 26.7\n",
            "episode  3 score 29.0 avg score 27.2\n",
            "episode  4 score 22.0 avg score 26.2\n",
            "episode  5 score 23.0 avg score 25.7\n",
            "episode  6 score 46.0 avg score 28.6\n",
            "episode  7 score 27.0 avg score 28.4\n",
            "episode  8 score 74.0 avg score 33.4\n",
            "episode  9 score 20.0 avg score 32.1\n",
            "episode  10 score 36.0 avg score 32.5\n",
            "episode  11 score 67.0 avg score 35.3\n",
            "episode  12 score 31.0 avg score 35.0\n",
            "episode  13 score 47.0 avg score 35.9\n",
            "episode  14 score 23.0 avg score 35.0\n",
            "episode  15 score 38.0 avg score 35.2\n",
            "episode  16 score 38.0 avg score 35.4\n",
            "episode  17 score 32.0 avg score 35.2\n",
            "episode  18 score 20.0 avg score 34.4\n",
            "episode  19 score 47.0 avg score 35.0\n",
            "episode  20 score 17.0 avg score 34.1\n",
            "episode  21 score 27.0 avg score 33.8\n",
            "episode  22 score 23.0 avg score 33.3\n",
            "episode  23 score 21.0 avg score 32.8\n",
            "episode  24 score 19.0 avg score 32.3\n",
            "episode  25 score 19.0 avg score 31.8\n",
            "episode  26 score 37.0 avg score 32.0\n",
            "episode  27 score 14.0 avg score 31.3\n",
            "episode  28 score 25.0 avg score 31.1\n",
            "episode  29 score 21.0 avg score 30.8\n",
            "episode  30 score 26.0 avg score 30.6\n",
            "episode  31 score 41.0 avg score 30.9\n",
            "episode  32 score 51.0 avg score 31.5\n",
            "episode  33 score 10.0 avg score 30.9\n",
            "episode  34 score 18.0 avg score 30.5\n",
            "episode  35 score 18.0 avg score 30.2\n",
            "episode  36 score 22.0 avg score 30.0\n",
            "episode  37 score 10.0 avg score 29.4\n",
            "episode  38 score 20.0 avg score 29.2\n",
            "episode  39 score 11.0 avg score 28.8\n",
            "episode  40 score 14.0 avg score 28.4\n",
            "episode  41 score 19.0 avg score 28.2\n",
            "episode  42 score 12.0 avg score 27.8\n",
            "episode  43 score 22.0 avg score 27.7\n",
            "episode  44 score 22.0 avg score 27.5\n",
            "episode  45 score 47.0 avg score 28.0\n",
            "episode  46 score 55.0 avg score 28.5\n",
            "episode  47 score 29.0 avg score 28.5\n",
            "episode  48 score 71.0 avg score 29.4\n",
            "episode  49 score 29.0 avg score 29.4\n",
            "episode  50 score 22.0 avg score 29.3\n",
            "episode  51 score 11.0 avg score 28.9\n",
            "episode  52 score 35.0 avg score 29.0\n",
            "episode  53 score 40.0 avg score 29.2\n",
            "episode  54 score 16.0 avg score 29.0\n",
            "episode  55 score 56.0 avg score 29.5\n",
            "episode  56 score 41.0 avg score 29.7\n",
            "episode  57 score 15.0 avg score 29.4\n",
            "episode  58 score 43.0 avg score 29.6\n",
            "episode  59 score 13.0 avg score 29.4\n",
            "episode  60 score 21.0 avg score 29.2\n",
            "episode  61 score 13.0 avg score 29.0\n",
            "episode  62 score 44.0 avg score 29.2\n",
            "episode  63 score 24.0 avg score 29.1\n",
            "episode  64 score 19.0 avg score 29.0\n",
            "episode  65 score 26.0 avg score 28.9\n",
            "episode  66 score 29.0 avg score 28.9\n",
            "episode  67 score 53.0 avg score 29.3\n",
            "episode  68 score 21.0 avg score 29.2\n",
            "episode  69 score 30.0 avg score 29.2\n",
            "episode  70 score 20.0 avg score 29.0\n",
            "episode  71 score 23.0 avg score 29.0\n",
            "episode  72 score 18.0 avg score 28.8\n",
            "episode  73 score 61.0 avg score 29.2\n",
            "episode  74 score 9.0 avg score 29.0\n",
            "episode  75 score 27.0 avg score 28.9\n",
            "episode  76 score 23.0 avg score 28.9\n",
            "episode  77 score 8.0 avg score 28.6\n",
            "episode  78 score 19.0 avg score 28.5\n",
            "episode  79 score 15.0 avg score 28.3\n",
            "episode  80 score 12.0 avg score 28.1\n",
            "episode  81 score 25.0 avg score 28.1\n",
            "episode  82 score 15.0 avg score 27.9\n",
            "episode  83 score 25.0 avg score 27.9\n",
            "episode  84 score 16.0 avg score 27.7\n",
            "episode  85 score 10.0 avg score 27.5\n",
            "episode  86 score 15.0 avg score 27.4\n",
            "episode  87 score 33.0 avg score 27.5\n",
            "episode  88 score 19.0 avg score 27.4\n",
            "episode  89 score 26.0 avg score 27.3\n",
            "episode  90 score 20.0 avg score 27.3\n",
            "episode  91 score 17.0 avg score 27.2\n",
            "episode  92 score 18.0 avg score 27.1\n",
            "episode  93 score 27.0 avg score 27.1\n",
            "episode  94 score 34.0 avg score 27.1\n",
            "episode  95 score 20.0 avg score 27.1\n",
            "episode  96 score 33.0 avg score 27.1\n",
            "episode  97 score 18.0 avg score 27.0\n",
            "episode  98 score 9.0 avg score 26.8\n",
            "episode  99 score 11.0 avg score 26.7\n",
            "episode  100 score 12.0 avg score 26.6\n",
            "episode  101 score 25.0 avg score 26.6\n",
            "episode  102 score 67.0 avg score 26.9\n",
            "episode  103 score 44.0 avg score 27.1\n",
            "episode  104 score 18.0 avg score 27.0\n",
            "episode  105 score 26.0 avg score 27.1\n",
            "episode  106 score 16.0 avg score 26.8\n",
            "episode  107 score 21.0 avg score 26.7\n",
            "episode  108 score 39.0 avg score 26.4\n",
            "episode  109 score 10.0 avg score 26.2\n",
            "episode  110 score 52.0 avg score 26.4\n",
            "episode  111 score 15.0 avg score 25.9\n",
            "episode  112 score 9.0 avg score 25.7\n",
            "episode  113 score 35.0 avg score 25.6\n",
            "episode  114 score 35.0 avg score 25.7\n",
            "episode  115 score 24.0 avg score 25.5\n",
            "episode  116 score 59.0 avg score 25.7\n",
            "episode  117 score 17.0 avg score 25.6\n",
            "episode  118 score 28.0 avg score 25.7\n",
            "episode  119 score 9.0 avg score 25.3\n",
            "episode  120 score 9.0 avg score 25.2\n",
            "episode  121 score 15.0 avg score 25.1\n",
            "episode  122 score 27.0 avg score 25.1\n",
            "episode  123 score 25.0 avg score 25.2\n",
            "episode  124 score 18.0 avg score 25.2\n",
            "episode  125 score 19.0 avg score 25.2\n",
            "episode  126 score 27.0 avg score 25.1\n",
            "episode  127 score 21.0 avg score 25.1\n",
            "episode  128 score 54.0 avg score 25.4\n",
            "episode  129 score 30.0 avg score 25.5\n",
            "episode  130 score 29.0 avg score 25.5\n",
            "episode  131 score 15.0 avg score 25.3\n",
            "episode  132 score 25.0 avg score 25.0\n",
            "episode  133 score 29.0 avg score 25.2\n",
            "episode  134 score 15.0 avg score 25.2\n",
            "episode  135 score 21.0 avg score 25.2\n",
            "episode  136 score 17.0 avg score 25.2\n",
            "episode  137 score 77.0 avg score 25.8\n",
            "episode  138 score 34.0 avg score 26.0\n",
            "episode  139 score 46.0 avg score 26.3\n",
            "episode  140 score 11.0 avg score 26.3\n",
            "episode  141 score 30.0 avg score 26.4\n",
            "episode  142 score 27.0 avg score 26.6\n",
            "episode  143 score 27.0 avg score 26.6\n",
            "episode  144 score 19.0 avg score 26.6\n",
            "episode  145 score 37.0 avg score 26.5\n",
            "episode  146 score 13.0 avg score 26.1\n",
            "episode  147 score 36.0 avg score 26.1\n",
            "episode  148 score 27.0 avg score 25.7\n",
            "episode  149 score 43.0 avg score 25.8\n",
            "episode  150 score 30.0 avg score 25.9\n",
            "episode  151 score 26.0 avg score 26.1\n",
            "episode  152 score 23.0 avg score 25.9\n",
            "episode  153 score 33.0 avg score 25.9\n",
            "episode  154 score 18.0 avg score 25.9\n",
            "episode  155 score 15.0 avg score 25.5\n",
            "episode  156 score 27.0 avg score 25.3\n",
            "episode  157 score 25.0 avg score 25.4\n",
            "episode  158 score 25.0 avg score 25.2\n",
            "episode  159 score 27.0 avg score 25.4\n",
            "episode  160 score 27.0 avg score 25.4\n",
            "episode  161 score 38.0 avg score 25.7\n",
            "episode  162 score 26.0 avg score 25.5\n",
            "episode  163 score 44.0 avg score 25.7\n",
            "episode  164 score 35.0 avg score 25.9\n",
            "episode  165 score 17.0 avg score 25.8\n",
            "episode  166 score 17.0 avg score 25.7\n",
            "episode  167 score 23.0 avg score 25.4\n",
            "episode  168 score 26.0 avg score 25.4\n",
            "episode  169 score 15.0 avg score 25.3\n",
            "episode  170 score 21.0 avg score 25.3\n",
            "episode  171 score 19.0 avg score 25.2\n",
            "episode  172 score 20.0 avg score 25.3\n",
            "episode  173 score 15.0 avg score 24.8\n",
            "episode  174 score 21.0 avg score 24.9\n",
            "episode  175 score 11.0 avg score 24.8\n",
            "episode  176 score 30.0 avg score 24.8\n",
            "episode  177 score 49.0 avg score 25.2\n",
            "episode  178 score 51.0 avg score 25.6\n",
            "episode  179 score 14.0 avg score 25.6\n",
            "episode  180 score 22.0 avg score 25.6\n",
            "episode  181 score 29.0 avg score 25.7\n",
            "episode  182 score 11.0 avg score 25.6\n",
            "episode  183 score 25.0 avg score 25.6\n",
            "episode  184 score 15.0 avg score 25.6\n",
            "episode  185 score 23.0 avg score 25.8\n",
            "episode  186 score 13.0 avg score 25.8\n",
            "episode  187 score 26.0 avg score 25.7\n",
            "episode  188 score 47.0 avg score 26.0\n",
            "episode  189 score 11.0 avg score 25.8\n",
            "episode  190 score 23.0 avg score 25.8\n",
            "episode  191 score 17.0 avg score 25.8\n",
            "episode  192 score 33.0 avg score 26.0\n",
            "episode  193 score 49.0 avg score 26.2\n",
            "episode  194 score 16.0 avg score 26.0\n",
            "episode  195 score 23.0 avg score 26.1\n",
            "episode  196 score 21.0 avg score 25.9\n",
            "episode  197 score 30.0 avg score 26.1\n",
            "episode  198 score 27.0 avg score 26.2\n",
            "episode  199 score 28.0 avg score 26.4\n",
            "episode  200 score 29.0 avg score 26.6\n",
            "episode  201 score 45.0 avg score 26.8\n",
            "episode  202 score 24.0 avg score 26.4\n",
            "episode  203 score 28.0 avg score 26.2\n",
            "episode  204 score 39.0 avg score 26.4\n",
            "episode  205 score 14.0 avg score 26.3\n",
            "episode  206 score 22.0 avg score 26.3\n",
            "episode  207 score 20.0 avg score 26.3\n",
            "episode  208 score 22.0 avg score 26.2\n",
            "episode  209 score 11.0 avg score 26.2\n",
            "episode  210 score 17.0 avg score 25.8\n",
            "episode  211 score 28.0 avg score 25.9\n",
            "episode  212 score 12.0 avg score 26.0\n",
            "episode  213 score 19.0 avg score 25.8\n",
            "episode  214 score 25.0 avg score 25.7\n",
            "episode  215 score 21.0 avg score 25.7\n",
            "episode  216 score 22.0 avg score 25.3\n",
            "episode  217 score 13.0 avg score 25.3\n",
            "episode  218 score 24.0 avg score 25.2\n",
            "episode  219 score 19.0 avg score 25.3\n",
            "episode  220 score 15.0 avg score 25.4\n",
            "episode  221 score 13.0 avg score 25.4\n",
            "episode  222 score 30.0 avg score 25.4\n",
            "episode  223 score 19.0 avg score 25.4\n",
            "episode  224 score 23.0 avg score 25.4\n",
            "episode  225 score 18.0 avg score 25.4\n",
            "episode  226 score 28.0 avg score 25.4\n",
            "episode  227 score 16.0 avg score 25.4\n",
            "episode  228 score 10.0 avg score 24.9\n",
            "episode  229 score 37.0 avg score 25.0\n",
            "episode  230 score 22.0 avg score 24.9\n",
            "episode  231 score 23.0 avg score 25.0\n",
            "episode  232 score 39.0 avg score 25.1\n",
            "episode  233 score 20.0 avg score 25.0\n",
            "episode  234 score 11.0 avg score 25.0\n",
            "episode  235 score 16.0 avg score 24.9\n",
            "episode  236 score 31.0 avg score 25.1\n",
            "episode  237 score 65.0 avg score 25.0\n",
            "episode  238 score 22.0 avg score 24.9\n",
            "episode  239 score 22.0 avg score 24.6\n",
            "episode  240 score 27.0 avg score 24.8\n",
            "episode  241 score 26.0 avg score 24.7\n",
            "episode  242 score 16.0 avg score 24.6\n",
            "episode  243 score 23.0 avg score 24.6\n",
            "episode  244 score 56.0 avg score 24.9\n",
            "episode  245 score 78.0 avg score 25.4\n",
            "episode  246 score 20.0 avg score 25.4\n",
            "episode  247 score 15.0 avg score 25.2\n",
            "episode  248 score 28.0 avg score 25.2\n",
            "episode  249 score 13.0 avg score 24.9\n"
          ]
        }
      ]
    }
  ]
}