{"cells":[{"cell_type":"markdown","metadata":{"id":"SOuPQQuQwtjP"},"source":["# Kuka\n","\n","---\n","\n","You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n","\n","### Start the Environment\n"]},{"cell_type":"markdown","metadata":{"id":"G7nSQa6ewtjV"},"source":["Make sure that you're in the right virtual environment and the right python version."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zy1jJwgwtjW","outputId":"3c83753d-98b2-48ae-b067-91d963620c0c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731271537295,"user_tz":300,"elapsed":11873,"user":{"displayName":"Kausar Patherya","userId":"09012208137279792936"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n","Collecting pybullet\n","  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pybullet\n","Successfully installed pybullet-3.2.6\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.1)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n","Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboardX\n","Successfully installed tensorboardX-2.6.2.2\n"]}],"source":["!python --version\n","!pip install pybullet\n","!pip install tensorboardX"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQR1NYtlwtjY","outputId":"d690b3e0-3ead-4482-d16a-90478e269467","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731271546447,"user_tz":300,"elapsed":9160,"user":{"displayName":"Kausar Patherya","userId":"09012208137279792936"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["current_dir=/usr/local/lib/python3.10/dist-packages/pybullet_envs/bullet\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n","  logger.warn(\n"]}],"source":["import matplotlib.pyplot as plt\n","import sys\n","from collections import deque, namedtuple\n","import timeit\n","from datetime import timedelta\n","from copy import deepcopy\n","import numpy as np\n","import random\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from tensorboardX import SummaryWriter\n","\n","import functools\n","import multiprocessing as mp\n","from multiprocessing import Pipe\n","from multiprocessing import Process\n","import signal\n","import warnings\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","import torch.optim as optim\n","from torch.distributions import Normal\n","\n","from gym import spaces\n","from pybullet_envs.bullet.kuka_diverse_object_gym_env import KukaDiverseObjectEnv\n","import pybullet as p"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAm4T-qewtjY"},"outputs":[],"source":["resize = T.Compose([T.ToPILImage(),\n","                    T.Resize(40, interpolation=Image.BICUBIC),\n","                    T.ToTensor()])\n","def worker(remote, env_fn):\n","    # Ignore CTRL+C in the worker process\n","    signal.signal(signal.SIGINT, signal.SIG_IGN)\n","    env = env_fn()\n","    try:\n","        while True:\n","            cmd, data = remote.recv()\n","            if cmd == 'step':\n","                ob, reward, done, info = env.step(data)\n","                remote.send((ob, reward, done, info))\n","            elif cmd == 'get_screen':\n","                screen = env._get_observation()\n","                screen = env._get_observation().transpose((2, 0, 1))\n","                screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n","                screen = torch.from_numpy(screen)\n","                screen = resize(screen).unsqueeze(0)\n","                remote.send(screen)\n","            elif cmd == 'reset':\n","                ob = env.reset()\n","                remote.send(ob)\n","            elif cmd == 'close':\n","                remote.close()\n","                break\n","            elif cmd == 'get_spaces':\n","                remote.send((env.action_space, env.observation_space))\n","            else:\n","                raise NotImplementedError\n","    finally:\n","        env.close()\n","\n","resize = T.Compose([T.ToPILImage(),\n","                    T.Resize(40, interpolation=Image.BICUBIC),\n","                    T.ToTensor()])\n","class MultiprocessVectorEnv:\n","    def __init__(self, env_fns):\n","        nenvs = len(env_fns)\n","        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n","        self.ps = \\\n","            [Process(target=worker, args=(work_remote, env_fn))\n","             for (work_remote, env_fn) in zip(self.work_remotes, env_fns)]\n","        for p in self.ps:\n","            p.start()\n","        self.last_obs = [None] * self.num_envs\n","        self.remotes[0].send(('get_spaces', None))\n","        self.action_space, self.observation_space = self.remotes[0].recv()\n","        self.closed = False\n","\n","    def __del__(self):\n","        if not self.closed:\n","            self.close()\n","\n","\n","    def step(self, actions):\n","        self._assert_not_closed()\n","        for remote, action in zip(self.remotes, actions):\n","            remote.send(('step', action))\n","        results = [remote.recv() for remote in self.remotes]\n","        self.last_obs, rews, dones, infos = zip(*results)\n","        return self.last_obs, rews, dones, infos\n","\n","    def get_screen(self):\n","        for remote in self.remotes:\n","            remote.send(('get_screen', None))\n","        results = [remote.recv() for remote in self.remotes]\n","        screens = torch.cat(results,dim=0)\n","        return screens\n","\n","    def reset(self, mask=None):\n","        self._assert_not_closed()\n","        if mask is None:\n","            mask = np.zeros(self.num_envs)\n","        for m, remote in zip(mask, self.remotes):\n","            if not m:\n","                remote.send(('reset', None))\n","\n","        obs = [remote.recv() if not m else o for m, remote,\n","               o in zip(mask, self.remotes, self.last_obs)]\n","        self.last_obs = obs\n","        return obs\n","\n","    def close(self):\n","        self._assert_not_closed()\n","        self.closed = True\n","        for remote in self.remotes:\n","            remote.send(('close', None))\n","        for p in self.ps:\n","            p.join()\n","\n","    @property\n","    def num_envs(self):\n","        return len(self.remotes)\n","\n","    def _assert_not_closed(self):\n","        assert not self.closed, \"This env is already closed\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cKi0U_0JwtjZ"},"outputs":[],"source":["def make_env(idx, test):\n","    env = KukaDiverseObjectEnv(renders=False, isDiscrete=False, removeHeightHack=False, maxSteps=20)\n","    env.observation_space = spaces.Box(low=0., high=1., shape=(84, 84, 3), dtype=np.float32)\n","    env.action_space = spaces.Box(low=-1, high=1, shape=(5,1))\n","    return env\n","\n","def make_batch_env(test):\n","    return MultiprocessVectorEnv(\n","        [functools.partial(make_env, idx, test)\n","            for idx in range(mp.cpu_count()*2)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHuTVMrZwtja"},"outputs":[],"source":["envs = make_batch_env(test=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJqocnu1wtja"},"outputs":[],"source":["# if gpu is to be used\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"ExlUp8Uhwtjb"},"source":["Actor-Critic implementation"]},{"cell_type":"code","source":["class Critic(nn.Module):\n","    def __init__(self, beta, state_dims, action_dims, fc1_dims, fc2_dims, name='Critic', ckpt_dir='tmp'):\n","        super(Critic, self).__init__()\n","        self.beta = beta\n","        self.state_dims = state_dims\n","        self.action_dims = action_dims\n","        self.fc1_dims = fc1_dims\n","        self.fc2_dims = fc2_dims\n","        self.name = name\n","        self.ckpt_path = os.path.join(ckpt_dir, name)\n","\n","        self.fc1 = nn.Linear(*(np.array(self.state_dims) + np.array(self.action_dims)), self.fc1_dims)\n","        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n","        self.q = nn.Linear(self.fc2_dims, 1)\n","\n","        self.optimizer = optim.Adam(self.parameters(), lr=self.beta)\n","        self.device = ('cuda:0' if T.cuda.is_available() else 'cpu')\n","        self.to(self.device)\n","\n","    def forward(self, state, action):\n","        state_action = T.cat([state, action], dim=1).to(self.device)\n","        x = F.relu(self.fc1(state_action))\n","        x = F.relu(self.fc2(x))\n","        q = self.q(x)\n","        return q\n","\n","    def save_checkpoint(self):\n","        print('... saving checkpoint ...')\n","        T.save(self.state_dict(), self.ckpt_path)\n","\n","    def load_checkpoint(self, gpu_to_cpu=False):\n","        print('... loading checkpoint ...')\n","        if gpu_to_cpu:\n","            self.load_state_dict(T.load(self.ckpt_path, map_location=lambda storage, loc: storage))\n","        else:\n","            self.load_state_dict(T.load(self.ckpt_path))\n","\n","\n","class Actor(nn.Module):\n","    def __init__(self, alpha, state_dims, action_dims, fc1_dims, fc2_dims, max_action, reparam_noise,\n","                 name='Actor', ckpt_dir='tmp'):\n","        super(Actor, self).__init__()\n","        self.alpha = alpha\n","        self.state_dims = state_dims\n","        self.action_dims = action_dims\n","        self.fc1_dims = fc1_dims\n","        self.fc2_dims = fc2_dims\n","        self.max_action = max_action\n","        self.name = name\n","        self.ckpt_path = os.path.join(ckpt_dir, name)\n","        self.reparam_noise = reparam_noise\n","\n","        self.fc1 = nn.Linear(*state_dims, self.fc1_dims)\n","        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n","        self.mu = nn.Linear(self.fc2_dims, *self.action_dims)\n","        self.sigma = nn.Linear(self.fc2_dims, *self.action_dims)\n","\n","        self.optimizer = optim.Adam(self.parameters(), lr=self.alpha)\n","        self.device = ('cuda:0' if T.cuda.is_available() else 'cpu')\n","        self.to(self.device)\n","\n","    def forward(self, state):\n","        x = F.relu(self.fc1(state))\n","        x = F.relu(self.fc2(x))\n","        mu = self.mu(x)\n","        sigma = self.sigma(x)\n","        sigma = T.clamp(sigma, min=self.reparam_noise, max=1.)\n","        return mu, sigma\n","\n","    def sample_normal(self, state, reparameterize=True):\n","        mu, sigma = self.forward(state)\n","        probs = Normal(mu, sigma)\n","        if reparameterize:\n","            actions = probs.rsample()\n","        else:\n","            actions = probs.sample()\n","        action = T.tanh(actions) * T.tensor(self.max_action).to(self.device)\n","        log_probs = probs.log_prob(actions)\n","        log_probs -= T.log(1 - action.pow(2) + self.reparam_noise)\n","        log_probs = log_probs.sum(1, keepdim=True)\n","        return action, log_probs\n","\n","    def save_checkpoint(self):\n","        print('... saving checkpoint ...')\n","        T.save(self.state_dict(), self.ckpt_path)\n","\n","    def load_checkpoint(self, gpu_to_cpu=False):\n","        print('... loading checkpoint ...')\n","        if gpu_to_cpu:\n","            self.load_state_dict(T.load(self.ckpt_path, map_location=lambda storage, loc: storage))\n","        else:\n","            self.load_state_dict(T.load(self.ckpt_path))\n","\n","\n","class Value(nn.Module):\n","    def __init__(self, beta, state_dims, fc1_dims, fc2_dims, name='Value', ckpt_dir='tmp'):\n","        super(Value, self).__init__()\n","        self.beta = beta\n","        self.state_dims = state_dims\n","        self.fc1_dims = fc1_dims\n","        self.fc2_dims = fc2_dims\n","        self.name = name\n","        self.ckpt_path = os.path.join(ckpt_dir, self.name)\n","\n","        self.fc1 = nn.Linear(*self.state_dims, self.fc1_dims)\n","        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n","        self.v = nn.Linear(self.fc2_dims, 1)\n","\n","        self.optimizer = optim.Adam(self.parameters(), lr=self.beta)\n","        self.device = ('cuda:0' if T.cuda.is_available() else 'cpu')\n","        self.to(self.device)\n","\n","    def forward(self, state):\n","        x = F.relu(self.fc1(state))\n","        x = F.relu(self.fc2(x))\n","        v = self.v(x)\n","        return v\n","\n","    def save_checkpoint(self):\n","        print('... saving checkpoint ...')\n","        T.save(self.state_dict(), self.ckpt_path)\n","\n","    def load_checkpoint(self, gpu_to_cpu=False):\n","        print('... loading checkpoint ...')\n","        if gpu_to_cpu:\n","            self.load_state_dict(T.load(self.ckpt_path, map_location=lambda storage, loc: storage))\n","        else:\n","            self.load_state_dict(T.load(self.ckpt_path))"],"metadata":{"id":"_smGMyu5QmkK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ReplayBuffer:\n","    def __init__(self, buffer_size, state_dims, action_dims):\n","        self.buffer_size = buffer_size\n","        self.ptr = 0\n","        self.is_full = False\n","\n","        # Init buffer\n","        self.states = np.zeros((self.buffer_size, *state_dims), dtype=np.float32)\n","        self.states_ = np.zeros((self.buffer_size, *state_dims), dtype=np.float32)\n","        self.actions = np.zeros((self.buffer_size, *action_dims), dtype=np.float32)\n","        self.rewards = np.zeros((self.buffer_size, ), dtype=np.float32)\n","        self.done = np.zeros((self.buffer_size,), dtype=np.bool)\n","\n","    def store_transition(self, state, action, reward, state_, done):\n","        self.states[self.ptr] = state\n","        self.actions[self.ptr] = action\n","        self.rewards[self.ptr] = reward\n","        self.states_[self.ptr] = state_\n","        self.done[self.ptr] = done\n","        self.ptr = (self.ptr + 1) % self.buffer_size\n","\n","        if self.ptr == 0 and not self.is_full:\n","            self.is_full = True\n","            print('... Replay Buffer is full ...')\n","\n","    def load_batch(self, batch_size):\n","        if self.is_full:\n","            samples = np.random.choice(np.arange(self.buffer_size), batch_size, replace=False)\n","        else:\n","            samples = np.random.choice(np.arange(self.ptr), batch_size, replace=False)\n","        states = self.states[samples]\n","        actions = self.actions[samples]\n","        rewards = self.rewards[samples]\n","        states_ = self.states_[samples]\n","        done = self.done[samples]\n","\n","        return states, actions, rewards, states_, done"],"metadata":{"id":"ECAN5dEiQhpP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVWw7gyAwtjc"},"outputs":[],"source":["class Agent:\n","    def __init__(self, gamma, alpha, beta, state_dims, action_dims, max_action, fc1_dim, fc2_dim,\n","                 memory_size, batch_size, tau, update_period, reward_scale, warmup, reparam_noise_lim,\n","                 name, ckpt_dir='tmp'):\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.state_dims = state_dims\n","        self.action_dims = action_dims\n","        self.max_action = max_action\n","        self.fc1_dim = fc1_dim\n","        self.fc2_dim = fc2_dim\n","        self.memory_size = memory_size\n","        self.batch_size = batch_size\n","        self.update_period = update_period\n","        self.tau = tau\n","        self.reward_scale = reward_scale\n","        self.warmup = warmup\n","        self.reparam_noise_lim = reparam_noise_lim\n","        self.name = name\n","        self.ckpt_dir = ckpt_dir\n","\n","        model_name = f'{name}__' \\\n","                     f'gamma_{gamma}__' \\\n","                     f'alpha_{alpha}__' \\\n","                     f'beta_{beta}__' \\\n","                     f'fc1_{fc1_dim}__' \\\n","                     f'fc2_{fc2_dim}__' \\\n","                     f'bs_{batch_size}__' \\\n","                     f'buffer_{memory_size}__' \\\n","                     f'update_period_{update_period}__' \\\n","                     f'tau_{tau}__'\n","\n","        self.model_name = model_name\n","        self.learn_iter = 0\n","        self.full_path = os.path.join(self.ckpt_dir, self.model_name)\n","\n","        # init replay buffer\n","        self.replay_buffer = ReplayBuffer(self.memory_size, self.state_dims, self.action_dims)\n","\n","        # init actor network\n","        self.actor = Actor(self.alpha, self.state_dims, self.action_dims, self.fc1_dim, self.fc2_dim, self.max_action,\n","                           self.reparam_noise_lim, name='Actor', ckpt_dir=self.ckpt_dir)\n","\n","        # init critic networks\n","        self.critic_1 = Critic(self.beta, self.state_dims, self.action_dims, self.fc1_dim, self.fc2_dim,\n","                               name='Critic_1', ckpt_dir=self.ckpt_dir)\n","        self.critic_2 = Critic(self.beta, self.state_dims, self.action_dims, self.fc1_dim, self.fc2_dim,\n","                               name='Critic_2', ckpt_dir=self.ckpt_dir)\n","\n","        # init value network\n","        self.value = Value(self.beta, self.state_dims, self.fc1_dim, self.fc2_dim,\n","                           name='Value', ckpt_dir=self.ckpt_dir)\n","\n","        # init target value network\n","        self.target_value = Value(self.beta, self.state_dims, self.fc1_dim, self.fc2_dim,\n","                                  name='Target_Value', ckpt_dir=self.ckpt_dir)\n","\n","        # hard network parameters update\n","        self.update_parameters(tau=1)\n","\n","    def choose_action(self, state, deterministic=False, reparameterize=False):\n","        state = T.tensor([state], dtype=T.float).to(self.actor.device)\n","        if deterministic:\n","            # deterministic action defined by the mean\n","            mu, _ = self.actor.forward(state)\n","            actions = mu\n","        else:\n","            # stochastic action is sampled from the normal distribution\n","            actions, _ = self.actor.sample_normal(state, reparameterize)\n","        return actions.cpu().detach().numpy()[0]\n","\n","    def store_transition(self, state, action, reward, state_, done):\n","        return self.replay_buffer.store_transition(state, action, reward, state_, done)\n","\n","    def load_batch(self):\n","        states, actions, rewards, states_, done = self.replay_buffer.load_batch(self.batch_size)\n","        states = T.tensor(states, dtype=T.float).to(self.actor.device)\n","        actions = T.tensor(actions, dtype=T.float).to(self.actor.device)\n","        rewards = T.tensor(rewards, dtype=T.float).to(self.actor.device)\n","        states_ = T.tensor(states_, dtype=T.float).to(self.actor.device)\n","        done = T.tensor(done, dtype=T.bool).to(self.actor.device)\n","        return states, actions, rewards, states_, done\n","\n","    def update_parameters(self, tau=None):\n","        # update the target value network parameters\n","        if tau is None:\n","            tau = self.tau\n","        value_state_parameters = self.value.named_parameters()\n","        target_value_state_parameters = self.target_value.named_parameters()\n","\n","        value_state_dict = dict(value_state_parameters)\n","        target_value_state_dict = dict(target_value_state_parameters)\n","\n","        for item in value_state_dict:\n","            value_state_dict[item] = tau * value_state_dict[item].clone() + \\\n","                                     (1 - tau) * target_value_state_dict[item].clone()\n","\n","        self.target_value.load_state_dict(value_state_dict)\n","\n","    def save_model(self):\n","        # saving all networks\n","        self.actor.save_checkpoint()\n","        self.critic_1.save_checkpoint()\n","        self.critic_2.save_checkpoint()\n","        self.value.save_checkpoint()\n","        self.target_value.save_checkpoint()\n","\n","    def load_model(self, gpu_to_cpu=False):\n","        # loading all networks\n","        self.actor.load_checkpoint(gpu_to_cpu=gpu_to_cpu)\n","        self.critic_1.load_checkpoint(gpu_to_cpu=gpu_to_cpu)\n","        self.critic_2.load_checkpoint(gpu_to_cpu=gpu_to_cpu)\n","        self.value.load_checkpoint(gpu_to_cpu=gpu_to_cpu)\n","        self.target_value.load_checkpoint(gpu_to_cpu=gpu_to_cpu)\n","\n","    def learn(self):\n","        # learns only after warmup iteration and when there is at least a single batch in buffer to load\n","        if self.learn_iter < self.warmup or self.learn_iter < self.batch_size:\n","            self.learn_iter += 1\n","            return\n","\n","        states, actions, rewards, states_, done = self.load_batch()\n","\n","        # optimize the value network\n","        actions_, log_probs = self.actor.sample_normal(states, reparameterize=False)\n","        log_probs = log_probs.view(-1)\n","        q1_new_policy = self.critic_1.forward(states, actions_)\n","        q2_new_policy = self.critic_2.forward(states, actions_)\n","        q_min = T.min(q1_new_policy, q2_new_policy).to(self.value.device)\n","        q_min = q_min.view(-1)\n","        v = self.value.forward(states).view(-1)\n","        v_ = self.target_value.forward(states_).view(-1)\n","        v_[done] = 0.0\n","\n","        target_value = q_min - log_probs\n","        value_loss = 0.5 * F.mse_loss(v, target_value)\n","        self.value.optimizer.zero_grad()\n","        value_loss.backward(retain_graph=True)\n","        self.value.optimizer.step()\n","\n","        # optimize the actor network\n","        actions_, log_probs = self.actor.sample_normal(states, reparameterize=True)\n","        log_probs = log_probs.view(-1)\n","        q1_new_policy = self.critic_1.forward(states, actions_)\n","        q2_new_policy = self.critic_2.forward(states, actions_)\n","        q_min = T.min(q1_new_policy, q2_new_policy).to(self.value.device)\n","        q_min = q_min.view(-1)\n","\n","        self.actor.optimizer.zero_grad()\n","        actor_loss = T.mean(log_probs - q_min)\n","        actor_loss.backward(retain_graph=True)\n","        self.actor.optimizer.step()\n","\n","        # optimize the critic networks\n","        q_ = self.reward_scale * rewards.view(-1) + self.gamma * v_\n","        q1_old_policy = self.critic_1.forward(states, actions).view(-1)\n","        q2_old_policy = self.critic_2.forward(states, actions).view(-1)\n","        critic_1_loss = 0.5 * F.mse_loss(q1_old_policy, q_)\n","        critic_2_loss = 0.5 * F.mse_loss(q2_old_policy, q_)\n","        critic_loss = T.add(critic_1_loss, critic_2_loss)\n","        self.critic_1.optimizer.zero_grad()\n","        self.critic_2.optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_1.optimizer.step()\n","        self.critic_2.optimizer.step()\n","\n","        # soft update networks parameters\n","        if self.learn_iter % self.update_period == 0:\n","            self.update_parameters()\n","        self.learn_iter += 1"]},{"cell_type":"markdown","metadata":{"id":"13lyOMd1wtjd"},"source":["Examine the state and action spaces."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2dekRgnwtjd","outputId":"4d3ceea5-52bf-44a2-f697-de4b870df0ed","colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"status":"ok","timestamp":1731271636239,"user_tz":300,"elapsed":3190,"user":{"displayName":"Kausar Patherya","userId":"09012208137279792936"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of agents: 24\n","Size of each action: 5\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLl0lEQVR4nO3deXyU9bU/8M/s2ScsWSEEAgqyBBQwBhUpIEuVi0LrQqvgtVAV6lVaq/SnIlpFbYsrgm0tqLdoxYJrBRUhVgUVCgIuKcTInkCA7Mms398f3EwZSXJOMPgE+Lxfr3m9yMxhnjPPPDNnJnnO99iMMQZERETfM7vVCRAR0emJBYiIiCzBAkRERJZgASIiIkuwABERkSVYgIiIyBIsQEREZAkWICIisgQLEBERWYIFiE4aa9asgc1mw5o1a6xO5bRks9lwzz33WJ0GnUJYgE4Rixcvhs1ma/Kybt06q1M85X3xxRe455578M0331iWw5IlS/Doo49atn2ilnBanQC1rnvvvRfdunU75voePXpYkM3p5YsvvsCcOXMwbNgwdO3a1ZIclixZgq1bt+KWW26xZPtELcECdIoZO3YsBg0aZHUaJDDGoL6+HrGxsVanctKoqalBfHy81WlQK+Kv4E4zs2fPht1ux6pVq6KunzZtGtxuNz777DMAgN/vx913342BAwfC6/UiPj4eF154IVavXh31/7755hvYbDb8/ve/x/z585GTk4O4uDiMGjUKu3btgjEG9913Hzp37ozY2FiMHz8ehw4dirqPrl274tJLL8Xbb7+NAQMGICYmBr1798ayZctUj+njjz/GmDFj4PV6ERcXh4suuggffvih6v/6fD7Mnj0bPXr0gMfjQVZWFn7961/D5/NFYiZPnoyYmBh8+eWXUf939OjRaNeuHfbu3YvFixfjxz/+MQDgBz/4QeRXnw1/r2p4jCtXrsSgQYMQGxuLp59+GgCwaNEiDB8+HKmpqfB4POjduzcWLFjQaL5vvfUWLrroIiQmJiIpKQmDBw/GkiVLAADDhg3Dm2++iR07dkS2f/Q3Mc1jbYi79dZbkZKSgsTERPzXf/0Xdu/erdqfAPDEE0+gT58+iIuLQ7t27TBo0KBIjg327NmD66+/HpmZmfB4POjWrRtuvPFG+P1+AP/5lXJBQQFuuukmpKamonPnzlH74cILL0R8fDwSExNxySWX4PPPPz8ml6+++go/+tGP0L59e8TExGDQoEF47bXXomIatvXhhx9i5syZSElJQXx8PC6//HIcOHBA/bjpOBg6JSxatMgAMO+++645cOBA1KWsrCwS5/f7zdlnn22ys7NNZWWlMcaYFStWGADmvvvui8QdOHDAZGRkmJkzZ5oFCxaYhx9+2PTs2dO4XC6zcePGSFxxcbEBYAYMGGB69+5t5s2bZ+68807jdrvNeeedZ37zm9+YIUOGmMcff9zcfPPNxmazmeuuuy4q9+zsbHPmmWea5ORkc8cdd5h58+aZfv36Gbvdbt5+++1I3OrVqw0As3r16sh1q1atMm632+Tn55s//OEP5pFHHjG5ubnG7Xabjz/+uNl9FgqFzKhRo0xcXJy55ZZbzNNPP21mzJhhnE6nGT9+fCTu8OHDpnPnzmbw4MEmGAwaY4xZuHChAWCef/55Y4wxRUVF5uabbzYAzG9+8xvz/PPPm+eff96UlJREHmOPHj1Mu3btzB133GEWLlwYeRyDBw82U6ZMMY888oh54oknzKhRowwA8+STTx7zHNtsNtO3b19z//33m/nz55uf/exn5pprrjHGGPP222+bAQMGmI4dO0a2v3z58hY9VmOM+elPf2oAmEmTJpknn3zSTJgwweTm5hoAZvbs2c3u0z/+8Y8GgPnRj35knn76afPYY4+Z66+/3tx8882RmD179pjMzMxILgsXLjR33XWXOeuss8zhw4cjjxWA6d27t7nooovME088YR588EFjjDHPPfecsdlsZsyYMeaJJ54wDz30kOnatatJTk42xcXFke1s3brVeL1e07t3b/PQQw+ZJ5980gwdOtTYbDazbNmyqP0KwJx99tlm+PDh5oknnjC//OUvjcPhMFdccUWzj5e+GxagU0TDi6ixi8fjiYrdsmWLcbvd5mc/+5k5fPiw6dSpkxk0aJAJBAKRmGAwaHw+X9T/O3z4sElLSzP//d//HbmuoQClpKSY8vLyyPWzZs0yAEz//v2j7vfqq682brfb1NfXR67Lzs42AMzf//73yHUVFRUmIyPDnH322ZHrvl2AwuGwOeOMM8zo0aNNOByOxNXW1ppu3bqZiy++uNl99vzzzxu73W7++c9/Rl3fUFw+/PDDyHUrV640AMxvf/tb8/XXX5uEhARz2WWXRf2/pUuXHlMgv/0YV6xYccxttbW1x1w3evRok5OTE/m5vLzcJCYmmry8PFNXVxcVe/Rjv+SSS0x2dvZxP9ZNmzYZAOamm26Kips0aZKqAI0fP9706dOn2Zhrr73W2O128+mnnx5zW8NjaTieL7jggkjRN8aYqqoqk5ycbKZOnRr1/0pKSozX6426fsSIEaZfv35Rx1o4HDZDhgwxZ5xxRuS6hm2NHDkyal/eeuutxuFwRB3X1Lr4K7hTzPz58/HOO+9EXd56662omL59+2LOnDn485//jNGjR6OsrAzPPvssnM7//EnQ4XDA7XYDAMLhMA4dOoRgMIhBgwbhX//61zHb/fGPfwyv1xv5OS8vDwDw05/+NOp+8/Ly4Pf7sWfPnqj/n5mZicsvvzzyc1JSEq699lps3LgRJSUljT7WTZs2Ydu2bZg0aRIOHjyIsrIylJWVoaamBiNGjMD777+PcDjc5L5aunQpzjrrLPTq1Svyf8vKyjB8+HAAiPp146hRo/Dzn/8c9957LyZMmICYmJjIr9C0unXrhtGjRx9z/dF/B6qoqEBZWRkuuugifP3116ioqAAAvPPOO6iqqsIdd9yBmJiYqP9vs9nEbWsf6z/+8Q8AwM033xz1/7UnNSQnJ2P37t349NNPG709HA7jlVdewbhx4xr9W+W3H8vUqVPhcDgiP7/zzjsoLy/H1VdfHfU4HA4H8vLyIo/j0KFDeO+993DFFVegqqoqEnfw4EGMHj0a27ZtO+YYnDZtWtT2L7zwQoRCIezYsUP12KnleBLCKebcc89VnYRw22234cUXX8Qnn3yCBx54AL179z4m5tlnn8Uf/vAHfPXVVwgEApHrGzvLrkuXLlE/NxSjrKysRq8/fPhw1PU9evQ45s3nzDPPBHDk70zp6enHbHPbtm0AjvyNpikVFRVo165do7dt27YNX375JVJSUhq9ff/+/VE///73v8err76KTZs2YcmSJUhNTW1yu41pbL8BwIcffojZs2dj7dq1qK2tPSZ/r9eLoqIiAEc+PBwP7WPdsWMH7HY7unfvHnV7z549Vdu5/fbb8e677+Lcc89Fjx49MGrUKEyaNAnnn38+AODAgQOorKxUP45v77OG57yhcH5bUlISAGD79u0wxuCuu+7CXXfd1Wjs/v370alTp8jP3z6GG46bbx+r1HpYgE5TX3/9deTFvGXLlmNu/9///V9MmTIFl112GW677TakpqbC4XBg7ty5kTfDox39KVVzvWmFSfAN325+97vfYcCAAY3GJCQkNPv/+/Xrh3nz5jV6+7eL58aNGyNv1Fu2bMHVV1/donwbO+OtqKgII0aMQK9evTBv3jxkZWXB7XbjH//4Bx555JFmv8G1REsf6/E666yzUFhYiDfeeAMrVqzA3//+dzz11FO4++67MWfOnBbf37f3WcP+eP755xv9UNLwbbsh7le/+lWj3zqBY1sTTuSxSo1jAToNhcNhTJkyBUlJSbjlllvwwAMP4Ec/+hEmTJgQiXn55ZeRk5ODZcuWRX0zmT179gnJqeET69Hb+ve//w0ATfbUNHxKT0pKwsiRI1u8ze7du+Ozzz7DiBEjxF9j1dTU4LrrrkPv3r0xZMgQPPzww7j88ssxePDgSIzmV2Hf9vrrr8Pn8+G1116L+gT+7bMNGx7r1q1bm+3paioH7WPNzs5GOBxGUVFR1LeewsJC1eMBgPj4eFx55ZW48sor4ff7MWHCBNx///2YNWsWUlJSkJSUhK1bt6rv79uPAwBSU1Obfc5zcnIAAC6X67iODfp+8G9Ap6F58+bho48+wh//+Efcd999GDJkCG688UaUlZVFYho+DR796e/jjz/G2rVrT0hOe/fuxfLlyyM/V1ZW4rnnnsOAAQMa/aQLAAMHDkT37t3x+9//HtXV1cfcLp1Ce8UVV2DPnj3405/+dMxtdXV1qKmpifx8++23Y+fOnXj22Wcxb948dO3aFZMnT446hbmhR6W8vLzZ7R6tsf1cUVGBRYsWRcWNGjUKiYmJmDt3Lurr66NuO/r/xsfHR/5udDyPdezYsQCAxx9/PCpGu7rCwYMHo352u93o3bs3jDEIBAKw2+247LLL8Prrr2P9+vXH/H/p28bo0aORlJSEBx54IOrXwg0anvPU1FQMGzYMTz/9NPbt29dkHFmL34BOMW+99Ra++uqrY64fMmQIcnJy8OWXX+Kuu+7ClClTMG7cOABH+iAGDBiAm266CS+99BIA4NJLL8WyZctw+eWX45JLLkFxcTEWLlyI3r17N/pm/12deeaZuP766/Hpp58iLS0Nf/nLX1BaWnrMG/HR7HY7/vznP2Ps2LHo06cPrrvuOnTq1Al79uzB6tWrkZSUhNdff73J/3/NNdfgpZdewg033IDVq1fj/PPPRygUwldffYWXXnop0rPz3nvv4amnnsLs2bNxzjnnADjSuzNs2DDcddddePjhhwEAAwYMgMPhwEMPPYSKigp4PJ5If09TRo0aBbfbjXHjxuHnP/85qqur8ac//QmpqalRb5xJSUl45JFH8LOf/QyDBw/GpEmT0K5dO3z22Weora3Fs88+C+BIUf7b3/6GmTNnYvDgwUhISMC4cePUj3XAgAG4+uqr8dRTT6GiogJDhgzBqlWrsH37dtXzOGrUKKSnp+P8889HWloavvzySzz55JO45JJLkJiYCAB44IEH8Pbbb+Oiiy7CtGnTcNZZZ2Hfvn1YunQpPvjgAyQnJzd5/0lJSViwYAGuueYanHPOObjqqquQkpKCnTt34s0338T555+PJ598EsCRE3IuuOAC9OvXD1OnTkVOTg5KS0uxdu1a7N69O9LzRhay7Pw7alXNnYYNwCxatMgEg0EzePBg07lz52NOLX3ssccMAPO3v/3NGHPkdNUHHnjAZGdnG4/HY84++2zzxhtvmMmTJ0ed5ttwGvbvfve7qPtrOGV66dKljeZ59Cm42dnZ5pJLLjErV640ubm5xuPxmF69eh3zfxvrAzLGmI0bN5oJEyaYDh06GI/HY7Kzs80VV1xhVq1aJe43v99vHnroIdOnTx/j8XhMu3btzMCBA82cOXNMRUWFqaysNNnZ2eacc86JOp3cmCOn6drtdrN27drIdX/6059MTk6OcTgcUbk2PMbGvPbaayY3N9fExMSYrl27moceesj85S9/MQCi+loaYocMGWJiY2NNUlKSOffcc80LL7wQub26utpMmjTJJCcnGwBRz5X0WBvU1dWZm2++2XTo0MHEx8ebcePGmV27dqlOw3766afN0KFDI89F9+7dzW233RZ1/8YYs2PHDnPttdealJQU4/F4TE5Ojpk+fXrk1P/GjpOjrV692owePdp4vV4TExNjunfvbqZMmWLWr18fFVdUVGSuvfZak56eblwul+nUqZO59NJLzcsvvxyJaWpbTR1v1HpsxvAvbGStrl27om/fvnjjjTesToWIvkf8GxAREVmCBYiIiCzBAkRERJbg34CIiMgS/AZERESWYAEiIiJLtLlG1HA4jL179yIxMfG4ljYhIiJrGWNQVVWFzMxM2O3NfM85UQ1GTz75ZKSJ8dxzzxWHgzVoaHjjhRdeeOHl5L7s2rWr2ff7E/INqGEpkIULFyIvLw+PPvooRo8ejcLCQnEJ+4blOp5/6H8QF+tpMq7Zqvp/bIoYADA2ecXhplbKPZrTGSPGOJS/9Awrzg1x2OWcHHb5W6TD7VLlZCDvJ5fDrbgn+X5s0H371TzFJiwHhRU5xcY0fTwezemQ92coGBRjdpfuF2O+3Hnsum+N+XxHuRiTEH/sat3fpnndaV4rAOAPhsSYkedkizE9szuIMaXl8v4GgL986Bdj/JCPg2qHnNOEbTNVOXUL7xJjYtzyW3mc4mlx6546eBzNvz6r/AbnvOSLvJ835YQUoHnz5mHq1Km47rrrAAALFy7Em2++ib/85S+44447mv2/Db92i4v1IL7ZAiTvKX0Bkl8IDoe8q5yuk7MAOd2aogEYyPvJ5VS8SZtWLECKF8z3XYBcigIUVBSg+Dj5eIqJqRdjAESGCzbH45EfX2sWIJtDPp7i4+SimBgfJ8bUBHQFyO2RczeQnxeXI16MiXPp3gziw/JrIdYlx8Qp3u0VD///4nSvT+nPKK1+EoLf78eGDRuilkC32+0YOXJkoysp+3w+VFZWRl2IiOjU1+oFqKysDKFQCGlpaVHXp6WlNTpaee7cufB6vZFLaw3GIiKits3y07BnzZqFioqKyGXXLvn3nUREdPJr9b8BdezYEQ6HA6WlpVHXl5aWNjpYzOPxqH73TEREp5ZW/wbkdrsxcOBArFq1KnJdOBzGqlWrkJ+f39qbIyKik9QJOQtu5syZmDx5MgYNGoRzzz0Xjz76KGpqaiJnxWkYmw2mmTMobIozbWyKM8COxLXOKd3hsHzmms2mPM1EcWq4UZxNFgrLT7EtJN8PANgUH1dCIflsI02DsV15JpXmjDOP4vQfh+L0xM3bdqpyqqipFWMOHa4RY9744AsxJrVjiiqnxMQkMUZz9looJJ+5tmPHN5qU4PMpjpXBXcQYp+J46tD82cARc38sn732UZH8enlztTy6PiZQpcrJoTjDTfNWp+nr17b+S/elXUPghBSgK6+8EgcOHMDdd9+NkpISDBgwACtWrDjmxAQiIjp9nbCleGbMmIEZM2acqLsnIqKTnOVnwRER0emJBYiIiCzBAkRERJZgASIiIkuwABERkSVYgIiIyBJtbiJqRLj5VfuNoiHOQNfMaLcpmjWN3FllwnJjXUix7D8AuF3y8vmqaRNybyzCYXlfHgmUQ2yKJebdTnl/u5SDSapq68SYpe98JsZoRh+8/eFmVU67Sw+LMWeecaYYM3b0WDHm66+/VuWkaf5tbLHgbysrOyDGfPTRh6qcgkHFCAy73Kz5/268WozxenWdqLU+OWbwmfL4h7iC18SYUL28LwHAaMZknKTDo/kNiIiILMECRERElmABIiIiS7AAERGRJViAiIjIEixARERkCRYgIiKyBAsQERFZggWIiIgs0WZXQrDZbLA3MwM6pFiZQPvgHA75vjQjmwMBeUUBTUf6kUDFeG/N5wfFggI2zaxtADbFsgoJCbFizMtvyeOKi/ccUuUUDMvd9Bu/kFcL0Dy/MYqOdABITpLHOmekpYoxmlUOXC55JDkAtG/fXoz57LNNYkxxsZyTJ0ZeVQIAYhXH3YqCjWLMjT8ZJ8YkJyeoctKsCqI55sJOxbGiXb1AsZqJaaWYsCJGc1+abQH8BkRERBZhASIiIkuwABERkSVYgIiIyBIsQEREZAkWICIisgQLEBERWYIFiIiILNFmG1Gdbiec7qbTCwQC4n0Yo6yviu4ro5h/bVc0mYYUo8QBwCjGVmsenl+xnxyq2d7Av7/ZK8Yse08ef11dI889rtHMRgZgFHPCE+PlEcp2h9yx6/f7VTmFQnJOmuPA55O3t3+/biR3Tk53McbplI+DmuoaeWOKJmoAsDfz+m6gaep97tX3xZjbrpebVQHA7ZEbe42mW1PznqJs1pSPJlWvqq5ZVXE/gJyTJmeA34CIiMgiLEBERGQJFiAiIrIECxAREVmCBYiIiCzBAkRERJZgASIiIkuwABERkSXabCOq3W6HvZkGSYfLLd6HUbZVBRUdWjGK6YWeOLnhsbqmXpMS9h44LMbEKSZ0PvPaB2JM6YFyTUqAotE2GJQbLDVNn+4Y5aGpeO40DXiaxlCXojkYAGyKxl6n4r58fvlYKd5RrMopM7OTGOPxyNNsPYpGzQ4dOqhy6t+/j7w9xX5a8+EnYsw3O/aocvrrozPFGJ/ideBTNC3blN2air5mVUxY8R6mbUQ1wotKur0BvwEREZElWr0A3XPPPbDZbFGXXr16tfZmiIjoJHdCfgXXp08fvPvuu//ZiPJXF0REdPo4IZXB6XQiPT1dFevz+eDz/WfhycrKyhOREhERtTEn5G9A27ZtQ2ZmJnJycvCTn/wEO3fubDJ27ty58Hq9kUtWVtaJSImIiNqYVi9AeXl5WLx4MVasWIEFCxaguLgYF154IaqqqhqNnzVrFioqKiKXXbt2tXZKRETUBrX6r+DGjh0b+Xdubi7y8vKQnZ2Nl156Cddff/0x8R6PBx7F6cRERHRqOeGnYScnJ+PMM8/E9u3bT/SmiIjoJHLCT0+rrq5GUVERrrnmmhb9v0AwiEAw2OTtTqfcEOd2yg2PAODyyLvhy6/3iTEhRfPV9p2lqpxWfvS5GONwyJ1lsZ4YMUY14RFAMNz08/GfnOTPNJqBmWFFQysAhI3cgedyyk3LUDTpBZXTbG1Gjjt0WG40dioaWuNj5eZRAKiqbvxX4Ec7f8gFYkyP7tliTOXhg6qcHIqzY22Kpk+norH5UGW1KqfPt8l/AujX5wwxJrHrWWJM9Z4vVTkFw/LxFFS8poJGsS+VzbFB4dDU5AOcgG9Av/rVr1BQUIBvvvkGH330ES6//HI4HA5cffXVrb0pIiI6ibX6N6Ddu3fj6quvxsGDB5GSkoILLrgA69atQ0pKSmtvioiITmKtXoBefPHF1r5LIiI6BXEtOCIisgQLEBERWYIFiIiILMECRERElmABIiIiS7AAERGRJdrsoJ74mBjExzbdxb/vYIV4H+u26MYVJ8bL3eTvffKFGFNdJ4/htSs6uwHA49at4iDRjJrW0nSca5YU0IxK1470tdkUn6EUuzys6ABXLLoAAPAH5eOgsvyAGJOeJo806dm9qyYlxCUkiDHxiYliTJanuxizYd9eVU6HDsmriyR5var7khwok1eeAIDbH1wsxsyZOUmMGThObrz/1+cfaFJCoEIeJx5UvDal1QsA/QoGTiFOubgKvwEREZE1WICIiMgSLEBERGQJFiAiIrIECxAREVmCBYiIiCzBAkRERJZgASIiIku02UbU/12xDm5X02O3D1fWivexbfd+1bZcdrmJy+2Sd5XHLccYTccjgLBiDK9RjNi1u+THprgbNU2frU21Qd1nI01jbyAgjxKP8XjEmNQOSaqcoGiObZ8s31cwWC/G+AIBVUqolhu39+2VGx5TUzqKMfX1daqU9uyVG1b3lsqv4WBQ3gcej2IsO4DiPfL21m78txhz3sB+YkxdvdywDABQ9JJ7FG8rDkWMU/leEBLuS7q9Ab8BERGRJViAiIjIEixARERkCRYgIiKyBAsQERFZggWIiIgswQJERESWYAEiIiJLtNlG1C3b98HRzJQ/TQNiQjMTVY9mFKMuW6tXM6wcq6npV7Vrpo8qJovalJ9DdFNK5ZxsiudOOTgWKR3kiZlx8fFijNMh74NgSNf06XHJTY9hVUOyvBOaa9Y+2oEDJfJ9xcoTUTXPS22drhF1QP9cMWbLF3LTZ11dpRjjUu6nhHj5PePlf8iTTAf0yhFj+k2+TZVT4R//nxgTUHR+Ou3y6zeofOFJk1NDnIhKRERtGQsQERFZggWIiIgswQJERESWYAEiIiJLsAAREZElWICIiMgSLEBERGSJNtuI6nE64HQ2nV5I08inapzUNWKGFA2kJiiPLnQoGh4BwO3RPDWa8aOaEG2brRwXFys3YXZoJ08DdSmaOQEgbOR97rLL+zIQlKem6ia5AnX1PjlIcWweOnRIjPF65UZcAAj45SZaTYOw2y03anbuIjdhAkBF+QF5e4pJtZpXud2ue93ZFB2U9XXyJNPbHnxGjFny6K9VOdUH5Jw8ij5bp2ZqqmpvAi5788dKkBNRiYioLWtxAXr//fcxbtw4ZGZmwmaz4ZVXXom63RiDu+++GxkZGYiNjcXIkSOxbdu21sqXiIhOES0uQDU1Nejfvz/mz5/f6O0PP/wwHn/8cSxcuBAff/wx4uPjMXr0aNTX13/nZImI6NTR4r8BjR07FmPHjm30NmMMHn30Udx5550YP348AOC5555DWloaXnnlFVx11VXfLVsiIjpltOrfgIqLi1FSUoKRI0dGrvN6vcjLy8PatWsb/T8+nw+VlZVRFyIiOvW1agEqKTmy5HtaWlrU9WlpaZHbvm3u3Lnwer2RS1ZWVmumREREbZTlZ8HNmjULFRUVkcuuXbusTomIiL4HrVqA0tPTAQClpaVR15eWlkZu+zaPx4OkpKSoCxERnfpatQB169YN6enpWLVqVeS6yspKfPzxx8jPz2/NTRER0UmuxWfBVVdXY/v27ZGfi4uLsWnTJrRv3x5dunTBLbfcgt/+9rc444wz0K1bN9x1113IzMzEZZdd1qLthM2RS9Pkjl3dCGnAZpPrsKZSBxXbCyhbhA0UHf7NrBTRIMYtryjgcukOgw6K8dd2zb50Nj1qPRKjHOmrWX2iPiivTOCrk2PqlK0EiUnyaGvNygT2ZkbSN9AcuwCQ1aWLGKMbWy0/Me3atVfcD3D4kLwSgqZ9w+WU89a+F0Do8AcAh2JVhaBiRPanW3X9kb16DRRjar/6RIxxKvJ2Kb+SSIszKBZvAHAcBWj9+vX4wQ9+EPl55syZAIDJkydj8eLF+PWvf42amhpMmzYN5eXluOCCC7BixQrExMhLeBAR0emjxQVo2LBhzX6asNlsuPfee3Hvvfd+p8SIiOjUZvlZcEREdHpiASIiIkuwABERkSVYgIiIyBIsQEREZAkWICIiskSbHclts9mEEcGa8citV181I8A1jYNJSfGq7bkVI6k9Lnl7cXFy/5VNOa5Y04BXW1Mr34+i8TU2NlaVUzAkN6LuLykVYzyx8ujn5ORkTUqqSfCJifKSU0bRFBkTq+uvq6urE2Mqq3aLMZq8/X55ZDUAlB4oE2MOV8ir47sVjaiwKWdEqxoo5efFpejo/P0fl2s2hhsnXCjGdK79UIzxuORj3Cm/nADI/bp+7f3owoiIiFoXCxAREVmCBYiIiCzBAkRERJZgASIiIkuwABERkSVYgIiIyBIsQEREZIk224gaCodhCzfTzaTp9tP0qgJIiJGbPmPi5AY8h0Ou5zaja4hze+ScKiorxBi7Q94JcbFxqpz89fLU0H379oox6enpYsyBSrkBEdA10cbHy48vrHheDh0+rMopuV07MSbY3LH9f5x2udG4qrJKlZNNcRyEA3LM/hK5ebSwcKsqp8pqOXdNk6mmjzoc1r0ZaPpQ7YoG4bCicV0Y+RzxyvufiTHzfvRjMWbvu38XYxwJmqm4gM3WfO71Id1j4zcgIiKyBAsQERFZggWIiIgswQJERESWYAEiIiJLsAAREZElWICIiMgSLEBERGSJNtuIarOZZpudOraTG0NjYnTTIjUtag6n3BRYWSU3hno8ukmfAb/c9NnOKzc8BgLy/YQVU0UBoLq6Wg5SNAgfPnRItT0Np1M+hF2Kpt762noxJsnrVeV0sExu1rTb5M9+KampYkytojkYAEr3HBRjfvJLeWJmQjv5uYtZo2u23vphezGmsnqXGGOD/NrU9K0DgF3RTK5pV41TTPStg+65K6uSj82n3/lCjBmjeKOrD+p2lKPZadVAgBNRiYioLWMBIiIiS7AAERGRJViAiIjIEixARERkCRYgIiKyBAsQERFZggWIiIgswQJERESWaLMrIXTO6AiXSzce9rvav3+/GKMZW52cLHfKV1cpVhMAUFpaKsakpqWJMVWK0dY+f4kqp9g4uVPe4ZC70oNBuU3arrgfALApVhSIjY0XY8rLFfupTF5NQMvlkVfp+OwzubvdYZdXeQCAeK/cBp+U4hdjYhPk5y4mVrcCSUJCohjjVqx0oRm3bbPpVmcIBoKK+5K3Vx2oFWM0q3gAQMgvPy/vbpNjvGfkiDE/jDmgysnnr2v2dv+JGsn9/vvvY9y4ccjMzITNZsMrr7wSdfuUKVNgs9miLmPGjGnpZoiI6BTX4gJUU1OD/v37Y/78+U3GjBkzBvv27YtcXnjhhe+UJBERnXpa/Cu4sWPHYuzYsc3GeDwepKenH3dSRER06jshJyGsWbMGqamp6NmzJ2688UYcPNj07859Ph8qKyujLkREdOpr9QI0ZswYPPfcc1i1ahUeeughFBQUYOzYsQg1seT/3Llz4fV6I5esrKzWTomIiNqgVj8L7qqrror8u1+/fsjNzUX37t2xZs0ajBgx4pj4WbNmYebMmZGfKysrWYSIiE4DJ7wPKCcnBx07dsT27dsbvd3j8SApKSnqQkREp74TXoB2796NgwcPIiMj40RvioiITiIt/hVcdXV11LeZ4uJibNq0Ce3bt0f79u0xZ84cTJw4Eenp6SgqKsKvf/1r9OjRA6NHj27RdoKBEGzNDMuOVYy89QXk5iwACAQCYkxpldysqhnVW6YY1wwAycnyuO3k5GQxpqZGbohLTpYbNQHAEyM3PdZUVokxfp/8vCgnKMOnGEldV6dptJWbC2vrdCOUg4oR58l2uak3JT1BjDmjr27Ee/5Y+fEldJCbpH118mdWV3yNKqeUVPlDaXVVZzFmb+luMSZsdI2ompHcij5UBOW3FITDyrnViu0hLG+wLjtfjPG5/63YGIDiTc3e7FeO9m5xAVq/fj1+8IMfRH5u+PvN5MmTsWDBAmzevBnPPvssysvLkZmZiVGjRuG+++6DxyO/4IiI6PTR4gI0bNgwGNN0dVu5cuV3SoiIiE4PXIyUiIgswQJERESWYAEiIiJLsAAREZElWICIiMgSLEBERGSJNjsRNRgIwNZMO2JAMU2wtFTTPAokxMmNmOGQ3MgWUsR4vcmalFTTYOvr68WYJK+8tFG9r/nphg0OHz4sxngUkz6NovE3GNY1Dvp9cnOoTzHl0q+ISc9IVeUUFyNPz+2akyLG5F4sN9CmdivXpISAT/6sGQrKHY+amPRuugbL9E7y1N+Ez+TjoOxNuemxvl7TzQk47PIkXs3r3K6Y1KttttYExrrl192K99aKMbvide8FV3dqvq+zzhgA8muT34CIiMgSLEBERGQJFiAiIrIECxAREVmCBYiIiCzBAkRERJZgASIiIkuwABERkSXabCOqzemAzdl0U1jp/gPifSTGyxMlASAxMVGMSVDEOJ1y82hpqdx8BwAORaNtdbU8wTLczOymBrGxchMbAISCcgNeaan8vMQomlXr/YqRkgDciobd7t27iTEG8mNrl+RV5eRyycMXa6vlqaEH9sj7IKWr7jOkMYpGTEXDo8Mp309ye93wyY5p8uMzrkoxZm9RVzHms407NCk1O+ssEhNWxCiOJ9VzAsCuGcFqk3PyKJpst9Tq3jNH+Zo/fuuVE1H5DYiIiCzBAkRERJZgASIiIkuwABERkSVYgIiIyBIsQEREZAkWICIisgQLEBERWYIFiIiILNFmV0JwOJxwOJrucm+XLHel1ylGVgNARUWFGOP3y+NloemQVnQsA0AoJI+Idis67quq5E7yA2XyqG0AcDjkTmrFLoBDsXrBmV2zNCnB7ZIP4TjFyHWHQ/4sVllRrkkJB8vk1SC8ye3FmI9el7f17jLFcQng4qvl/dTrbLcYEwjK29v6gW4lhMwsuev+623y81JbJeftTZZXMgGA8sPlcpBi3LbLodiXYfk1DgB2m+J1F5bHoNsUKyrEKFZwAIAXDjQ/Uv7I2PJd4v3wGxAREVmCBYiIiCzBAkRERJZgASIiIkuwABERkSVYgIiIyBIsQEREZAkWICIiskSbbUR1OR1wNTOS2ygaBzUjsgFg74F9YkxdvTxCOTOjkxjjr/frctonj+42iqnVTrf8FGv3k80u7/MzeuSIMUlJcgOipmkOAHyKZuPioiIxxmGXt2dXNOICgD8gNxgeqpAbhOt9cnOh3Raryumj5e3EmC/WyKPS085cK8aU7pKbbAEg+6xPxZiag/liTIynoxjTOUP3VhdWHE+VdXIzrjHyc6fq2gZg7Jrx3vL9qCZ7K5psAeCwv/m4cFjX0Nqib0Bz587F4MGDkZiYiNTUVFx22WUoLCyMiqmvr8f06dPRoUMHJCQkYOLEiSgtld9MiYjo9NKiAlRQUIDp06dj3bp1eOeddxAIBDBq1CjU1Pzn28Gtt96K119/HUuXLkVBQQH27t2LCRMmtHriRER0cmvRr+BWrFgR9fPixYuRmpqKDRs2YOjQoaioqMAzzzyDJUuWYPjw4QCARYsW4ayzzsK6detw3nnntV7mRER0UvtOJyE0LOLZvv2R3/lu2LABgUAAI0eOjMT06tULXbp0wdq1jf/u2OfzobKyMupCRESnvuMuQOFwGLfccgvOP/989O3bFwBQUlICt9uN5OTkqNi0tDSUlJQ0ej9z586F1+uNXLKydKsgExHRye24C9D06dOxdetWvPjii98pgVmzZqGioiJy2bVLXsKbiIhOfsd1GvaMGTPwxhtv4P3330fnzp0j16enp8Pv96O8vDzqW1BpaSnS09MbvS+PxwOPRzc/hIiITh0t+gZkjMGMGTOwfPlyvPfee+jWrVvU7QMHDoTL5cKqVasi1xUWFmLnzp3Iz5fP5yciotNHi74BTZ8+HUuWLMGrr76KxMTEyN91vF4vYmNj4fV6cf3112PmzJlo3749kpKS8Itf/AL5+fktPgOuorIKTmfT6TkUDVMHKstU2+oZ01uMiXPGiTFFiobHClOlyqmTS/5b2KEk+fEle+UGxKzMNFVOQcWU1spK+fEFAnIHra9eN+nzQNl+OUjRpVfrlx+bJ0bX9OkLyE14YUVOnRSNzS6Xrjm27NBBMeZAqdxsXbr/DDHm4qs/VOVkjNwZmdhR7iGMiZPfC2LCfVU5JSbJr/PVBR+IMbEx8m917IrGbi0b5ONJ1xyqawC324VGW6NrRG1RAVqwYAEAYNiwYVHXL1q0CFOmTAEAPPLII7Db7Zg4cSJ8Ph9Gjx6Np556qiWbISKi00CLCpBRfGqLiYnB/PnzMX/+/ONOioiITn1cjJSIiCzBAkRERJZgASIiIkuwABERkSVYgIiIyBIsQEREZIk2OxF1x859cDQzgTIIuVGxj6ufaluTXT8XY9xheVrkfq88WfXrgNysCgBOxVDJgtQVYkxdiTzhsb5OjgF0jaj1dbViTFyc3OznaKYJ+Wg11fL22nfoLMZkdpIbf3fs+EyVU1y8PPE1I1We4ulyy5Nqk5MSVTllpKWIMYfqPxJjOnaqkzdm0zXHahqEY+PkpmUH5AbazompqpQ6tEsSY7Zu2SrGlCsasu123X5yKibx+kNy46fdJu9vo2hoBYBwuPmG1bBy2iu/ARERkSVYgIiIyBIsQEREZAkWICIisgQLEBERWYIFiIiILMECRERElmABIiIiS7AAERGRJdrsSgjGhJud6hpWjI6tCFeqtlVcVyzGdHPliDGpbnm0daZTHrMMADsDck5Ddo0UY/6VKHe37y+Txx4DgEPRua1oyMauXfKKETaH7rNRXFyyGDP8B/JKFyUl/xZjyvZ/pUkJMW75ZZWeKnfmexWrHLg88moJAOCwyyOi09otE2Nikg6JMeGQ7m1FMZkdnlh5xRObqRZjfAHdiPeevXqJMePH+cWY195cKcYcLte9PwUUK0YYxQhso1ihwq6byA2Pp/njKRQSRnY3bE+3OSIiotbFAkRERJZgASIiIkuwABERkSVYgIiIyBIsQEREZAkWICIisgQLEBERWaLNNqLabXbY7U3XR5eiz2mH/xvVtt7IfEmM6ezNFGMmHposxtgUDbQAkBXuKsZ0DnUTYzIq5HHUy5L+V5MSvt65Q4yJFRrUAMDlcosxNqP7bHTZ5b8RY1JS5Cbi4m/+JcbEx8WqcurcWd7ncfHyfcXGx4sxTqdurLMNcsNqyCmPnffXy/djU36stYXl5snkNE2T9H4xwveN7nVX9HWRGJOWkS7GJCbITcQVlfIocUA1uVw93lsSUo7Srq2rb/b2sOK5BfgNiIiILMICRERElmABIiIiS7AAERGRJViAiIjIEixARERkCRYgIiKyBAsQERFZos02oobDIdjQdFOUUTR0uuxywyMAfLV7uxjjTJJr9R/iZosxHXbLTWwAMDX5ZjFmW1Ce0FkdkpvdSmrlRj4ASEpIEGN8vloxZtTFt4ox3bqdo8opPV1uxq2ulqd4JiXITZ8mNUOVU8dUuWnZ45KPJ6dTbvqMVzSrAlB91HTWycdcfHyyGFPr3yRvDEC57WU5SDHV2ITkps/de8s0KcHjkptxvyneLcYMu2ioGLPslTdUOVVWV4kxdlX3r6LJ1ChHotqk+9I1tLboG9DcuXMxePBgJCYmIjU1FZdddhkKCwujYoYNGwabzRZ1ueGGG1qyGSIiOg20qAAVFBRg+vTpWLduHd555x0EAgGMGjUKNTXRn7KnTp2Kffv2RS4PP/xwqyZNREQnvxb9Cm7FihVRPy9evBipqanYsGEDhg79z1fOuLg4pKfrftVERESnp+90EkJFRQUAoH379lHX//Wvf0XHjh3Rt29fzJo1C7W1Tf9dwOfzobKyMupCRESnvuM+CSEcDuOWW27B+eefj759+0aunzRpErKzs5GZmYnNmzfj9ttvR2FhIZYtW9bo/cydOxdz5sw53jSIiOgkddwFaPr06di6dSs++OCDqOunTZsW+Xe/fv2QkZGBESNGoKioCN27dz/mfmbNmoWZM2dGfq6srERWVtbxpkVERCeJ4ypAM2bMwBtvvIH3339fnH2Sl5cHANi+fXujBcjj8cCjmCFDRESnlhYVIGMMfvGLX2D58uVYs2YNunWTezA2bdoEAMjI0PVQEBHR6aFFBWj69OlYsmQJXn31VSQmJqKkpAQA4PV6ERsbi6KiIixZsgQ//OEP0aFDB2zevBm33norhg4ditzc3JYl5nLA4Wh6yl8oJI9EVQ7lg1FMVz1QKjey7T90QIxxxegaB/9Y9YQY85ltvRgT45Inb8a45eY7AAiE5B0aFyc3q9bWlogx/kDzExcbFBa+J8ZUV+8RY4L+nWJMWseAKqekmK/lIPdAMcRA3l51tW6qZvK3ThRqYoMih0M+bynZNViREVBamCbGVNpeke/ILzctBwK6Bsvycvl13qGdV4ypVTSPds6UHz8AFG6Xn2O7XX58Drvm7V63n0LCm6vNpnvzbVEBWrBgAYAjzaZHW7RoEaZMmQK32413330Xjz76KGpqapCVlYWJEyfizjvvbMlmiIjoNNDiX8E1JysrCwUFBd8pISIiOj1wMVIiIrIECxAREVmCBYiIiCzBAkRERJZgASIiIkuwABERkSVYgIiIyBJtdiS33x+E3d50N63D3kojaAGEgvJSCPsPyGOd4+Li5PuJlbvyAWBHvdxN74a8PY9dHutsF8frHqEZyZ3sTRJjdn7zTzHG5dR1ZLsV6wiW7t0sxiQnyZ3rXVPlVRcAwITkkSJ+m0+MccWNEGNsdt1zFw7KqypoXlPhsPxaMeGgKievt+mVThrs+lJeVaGqTn7uYly6t7oOHeQVIzQjY5IS5NVF+vTto8ppxy75PSOseK/TjO12KPeT3db8c6dZqQbgNyAiIrIICxAREVmCBYiIiCzBAkRERJZgASIiIkuwABERkSVYgIiIyBIsQEREZIk224gaDgfRXH0Mh+VGRalZqoFN0fNoUwSFFTPAa2t0Y50BOXejGMPbUdFYFwzockpITBRjYmPkEeAJ8XIDbfnhL1Q5JbfrLcbEx5SLMdkpa8QYt1M3JrwuKL+s2sd9Ksb4PJeKMU6XrhHV7/eLMZXl5WJMnOK5Q1iXU2pqihiTqGjCrKiSj9+gogkTAHyK/VRSUirGbC47oNiartkaite52yk3nLvccnPsj8bmqVLqlZPV7O21dfW4dsa94v3wGxAREVmCBYiIiCzBAkRERJZgASIiIkuwABERkSVYgIiIyBIsQEREZAkWICIiskSbbUSN8cTC4Wi6GdPnkydKOpy6+urXNGIqYgIheRJkvGJqKgA4nfJT06F9snw/zezDBu2T5fsBdBMVbQ7NVE25YdcY3UTFTt5HxZiYFPl5CfjlJtNQWNfY7HTKj+/gYfl+au0lYkxSUrwmJSQqmojjE+T7Cvjl14H2GPf75ddwO8WxuWefPK24XrEtANj0z01iTFAx7TMUlI+55t7fjpaS0kGM6XNG842hAHDN5cPEmGSvfJwAgMfd/O3VNXWq++E3ICIisgQLEBERWYIFiIiILMECRERElmABIiIiS7AAERGRJViAiIjIEixARERkiTbbiNohOQFOZ9ONWrv3yY2DwZBu0qemCmuaJ+2Ke3K7PYqtAW63/NT4fPL0xvg0eeqk0yV0lUXi5Ma5oE/RsKvIO6ycZrtt13liTP8eK+Q7UmxP0VsIAAgoAqvr5GZNv0Nu5ktOVDZ9BuR9HhcvNyHa4uXXweFDcmMoAJSVHRRjNmzcKsZ8uW2bvDHdkFYEg3KTaUa63BjaOSNNjLEpk/rltPFijNslv184nYqpzopmegDYe6j5+6qp1TWS8xsQERFZokUFaMGCBcjNzUVSUhKSkpKQn5+Pt956K3J7fX09pk+fjg4dOiAhIQETJ05Eaak8P52IiE4/LSpAnTt3xoMPPogNGzZg/fr1GD58OMaPH4/PP/8cAHDrrbfi9ddfx9KlS1FQUIC9e/diwoQJJyRxIiI6ubXob0Djxo2L+vn+++/HggULsG7dOnTu3BnPPPMMlixZguHDhwMAFi1ahLPOOgvr1q3Deec1/rt6n88XtbBoZWVlSx8DERGdhI77b0ChUAgvvvgiampqkJ+fjw0bNiAQCGDkyJGRmF69eqFLly5Yu3Ztk/czd+5ceL3eyCUrS17VlYiITn4tLkBbtmxBQkICPB4PbrjhBixfvhy9e/dGSUkJ3G43kr+1fHpaWhpKSppeVn7WrFmoqKiIXHbt2tXiB0FERCefFp+G3bNnT2zatAkVFRV4+eWXMXnyZBQUFBx3Ah6PBx6P7tRkIiI6dbS4ALndbvTo0QMAMHDgQHz66ad47LHHcOWVV8Lv96O8vDzqW1BpaSnS09NbLWEiIjo1fOc+oHA4DJ/Ph4EDB8LlcmHVqlWR2woLC7Fz507k5+d/180QEdEppkXfgGbNmoWxY8eiS5cuqKqqwpIlS7BmzRqsXLkSXq8X119/PWbOnIn27dsjKSkJv/jFL5Cfn9/kGXDN+cml+YiNafpXc3OeWibeh9Po6qtdMUY6pBjDqxnVqz3Lr0MHudvarxiPXF8nd9N7XC5VTr46uUs6FJb3gdGM5FZ+Noptt1++LyOvchAOy13pYejGOlfV9RBjKv1jxZi0tAQxxhMfq8opqDhW/Da5U/7f27aLMV99WajK6V+btshBdvk4cClWAUhK1I0uPydXfu7OH3iWGJN/dk8xxh/QrdRiID8vxsjHbzCgWM3FLscAwOIPm189xe/TvX5bVID279+Pa6+9Fvv27YPX60Vubi5WrlyJiy++GADwyCOPwG63Y+LEifD5fBg9ejSeeuqplmyCiIhOEy0qQM8880yzt8fExGD+/PmYP3/+d0qKiIhOfVwLjoiILMECRERElmABIiIiS7AAERGRJViAiIjIEixARERkiTY7kjsUNAgFm2uKkpuzNKORAcCtaMTUjOSGohms3qdrZqysrBJj4uJixJiDh8vFGFczo8+PZrfLcTa7/LzU1cv7wAHdc9e/x2diTHWd/NxV1MjLRYXQSZWT5jiIjZVfepoxy4WFcmMoAOzZ1/SCwA2CisbItZ9sEGM8bt2Id7dHPn4vGCQ3fXbvmiHGeJWNqMPP6y3GBBWTtGsVY+eheU8BNG91sGtG2CsajV0OxcYAxMU3Pwpe+ZbCb0BERGQNFiAiIrIECxAREVmCBYiIiCzBAkRERJZgASIiIkuwABERkSVYgIiIyBJtthG1Y3I84mKbnog65oJc8T5W/lNuUgQAA7mzTBPjcMi7U9scGwjIzZrBkLy9/QcPizGxbt1E1Lg4efqmZnKsXTHl0ubSdbLV+xWNc0ZusKyuTxFjyqoGKDIC0trtEGP2le4VY9Z88KkYo2k0BoBDh+TjwKFoIo5PTBRjcjrL+xIAxg0fKMb06dlNjPEmyQ2twWab2v+jRtFAqmnI1hzjWpomeEVvLDwe+fld9ZXudVde3/x7RsCne0/hNyAiIrIECxAREVmCBYiIiCzBAkRERJZgASIiIkuwABERkSVYgIiIyBIsQEREZIk224jqcbvgaaZB8sxseQri6k++VG0r4FdM6HQoGrQUkzCdylGBfr/cPGmrqRNjNNNODxwq16QEV1W1GGMUTXOZ6any/RjdZMb3Phksxjgc8ucsY5cnZvqD+1U5fbxhnxhTU1MoxtTW1YsxKR3aqXLK6dZFESU/dzOuGSXGJCoalgEgOUmOCyoam2uq5f1ks2laNQGbYmooFMdmOCTvS5tN9/nfpplErHjvsRs5p13lTTf/H81vhEZU4fZITqooIiKiVsYCRERElmABIiIiS7AAERGRJViAiIjIEixARERkCRYgIiKyBAsQERFZggWIiIgs0WZXQqj3BZoda9uneyfxPkac11u1rVfeXS/GaKZWa1ZLCCtHcmv6tkMhebUEzSoANYqOewBo55HHMXds116M0YwrDod1neuHKt1iTJlibPWePf8WY9xueVsAYBSf62Lj5I7znj2yxZihg3qpchoysKcY41OsCOJWjJ23a1YNgW5MtmZhAl9AHqPt9uieO0crjdIOhuTXuVO5nzSrKmjyrvYpdqZdtxKCyym8IYZ0paVFe3vBggXIzc1FUlISkpKSkJ+fj7feeity+7Bhw2Cz2aIuN9xwQ0s2QUREp4kWfQPq3LkzHnzwQZxxxhkwxuDZZ5/F+PHjsXHjRvTp0wcAMHXqVNx7772R/xMXF9e6GRMR0SmhRQVo3LhxUT/ff//9WLBgAdatWxcpQHFxcUhPT2+9DImI6JR03L/wDIVCePHFF1FTU4P8/PzI9X/961/RsWNH9O3bF7NmzUJtbW2z9+Pz+VBZWRl1ISKiU1+LT0LYsmUL8vPzUV9fj4SEBCxfvhy9ex/5Y/+kSZOQnZ2NzMxMbN68GbfffjsKCwuxbNmyJu9v7ty5mDNnzvE/AiIiOim1uAD17NkTmzZtQkVFBV5++WVMnjwZBQUF6N27N6ZNmxaJ69evHzIyMjBixAgUFRWhe/fujd7frFmzMHPmzMjPlZWVyMrKOo6HQkREJ5MWFyC3240ePXoAAAYOHIhPP/0Ujz32GJ5++uljYvPy8gAA27dvb7IAeTweeDy6U/+IiOjU8Z1Peg+Hw/D5Gu8f2LRpEwAgI0OeXkpERKeXFn0DmjVrFsaOHYsuXbqgqqoKS5YswZo1a7By5UoUFRVhyZIl+OEPf4gOHTpg8+bNuPXWWzF06FDk5ua2euKhsDyq1+3SfbNyS01V0I30DQXlnFSdddB9MtCMrQ4pmtgU03wBALX1csNfMHhIjImJiRFjlCmhqHiXGONyyc9vjKJdQDWuGcDg3B5iTEaKPEr78osHiTFBzTEHXTOjyyHvJ83xBJsiBoDDKb/9hDQNnU5Fk6mysdkoxpJDMUq7NZutnU7F9iDvp/e2ya+7wgO6ht1kb/PHii2sG8ndogK0f/9+XHvttdi3bx+8Xi9yc3OxcuVKXHzxxdi1axfeffddPProo6ipqUFWVhYmTpyIO++8syWbICKi00SLCtAzzzzT5G1ZWVkoKCj4zgkREdHpgYuREhGRJViAiIjIEixARERkCRYgIiKyBAsQERFZggWIiIgs0WYnooZCQYRCTU8MNJCnCZ7ZJUW1rbQO8qTPvQfKxRi7ZuCgookNgKphNRTSNSFK3B5d01hFhbxSuaY5VjPB0mnXTYv0xMaKMT26yCtxDM3rK8bU1tapcvpB3lliTEyMvM/rfPJ+cuh6Y+HQNE8qYsKKw9duU076DCsmomqOA8VrJWx0zbE2zUtK866p6TFVdoAHA3JSCbHyExOnOOacLl0jqtRoq2nEBfgNiIiILMICRERElmABIiIiS7AAERGRJViAiIjIEixARERkCRYgIiKyBAsQERFZos02otrsdtiaaWYKh+Xms95ndFZtq0unVDFm13550qfDLu9Oox0/qp4J2rxAINBqm9JModVMH23fTp4GGhcrT28EgKsvHSJvzxsvxnTPThdjFENxAQCBoNz06A/IMXbF58OQtsFS1fQpv6bsqgZh3Y7S7E/N6yWsOC7tmi5xAEaRlEOzPYf8XhDWvhcYedrp9v3y49u4S24yjXHrmtJtwnuddHsDfgMiIiJLsAAREZElWICIiMgSLEBERGQJFiAiIrIECxAREVmCBYiIiCzBAkRERJZgASIiIku02ZUQHA4nHM10E2s6pGvq5JHGADBh1GAxZnepvBLCnpIDYoxd2SFsFN3WNsUoYs1oXIdTl1NKB3nEeTAkd9xPHHOuGNOra6Yqp3aJ8kjucFg+VmprFceKcvy1puneKJ47o1hRQDNqGwCCmvHtinHqdkWMX9G5DwB2xT5wOuSVFzTHrwkrVgSBbsx9WPEEq6Z2K5+7GMXjq6iXY0prNKuUKFcwEFZ6sCueN4DfgIiIyCIsQEREZAkWICIisgQLEBERWYIFiIiILMECRERElmABIiIiS7S5PqCG/p7ael+zcZoeGIdTdy66tC1A10cRVkydBHQTLI1mgqViH9gUzSshRe8OAASD8j4IKfKu98k9N7V19aqc3A758Wn6gIymyef77gNS9Lo5FPcD6I5Np6JHTdNXpp3SqusDkrdn1/SxKfuANPtJMznW4ZR7oYzy83/QKW+vrk7eBwFfrRjjr5enpgKAMc2/F/jra/4vrvlj2Gb0M6K/F7t370ZWVpbVaRAR0Xe0a9cudO7cucnb21wBCofD2Lt3LxITEyOf8CsrK5GVlYVdu3YhKSnJ4gz1mPf372TNnXl/v5j3iWWMQVVVFTIzM5v91tzmfgVnt9ubrJhJSUlteqc3hXl//07W3Jn394t5nzher1eM4UkIRERkCRYgIiKyxElRgDweD2bPng2Px2N1Ki3CvL9/J2vuzPv7xbzbhjZ3EgIREZ0eTopvQEREdOphASIiIkuwABERkSVYgIiIyBIsQEREZIk2X4Dmz5+Prl27IiYmBnl5efjkk0+sTkl0zz33wGazRV169epldVrHeP/99zFu3DhkZmbCZrPhlVdeibrdGIO7774bGRkZiI2NxciRI7Ft2zZrkj2KlPeUKVOO2f9jxoyxJtmjzJ07F4MHD0ZiYiJSU1Nx2WWXobCwMCqmvr4e06dPR4cOHZCQkICJEyeitLTUooyP0OQ9bNiwY/b5DTfcYFHGRyxYsAC5ubmRVQPy8/Px1ltvRW5vi/u6gZR7W9zfx6NNF6C//e1vmDlzJmbPno1//etf6N+/P0aPHo39+/dbnZqoT58+2LdvX+TywQcfWJ3SMWpqatC/f3/Mnz+/0dsffvhhPP7441i4cCE+/vhjxMfHY/To0aiv161UfaJIeQPAmDFjovb/Cy+88D1m2LiCggJMnz4d69atwzvvvINAIIBRo0ahpqYmEnPrrbfi9ddfx9KlS1FQUIC9e/diwoQJFmatyxsApk6dGrXPH374YYsyPqJz58548MEHsWHDBqxfvx7Dhw/H+PHj8fnnnwNom/u6gZQ70Pb293Exbdi5555rpk+fHvk5FAqZzMxMM3fuXAuzks2ePdv079/f6jRaBIBZvnx55OdwOGzS09PN7373u8h15eXlxuPxmBdeeMGCDBv37byNMWby5Mlm/PjxluTTEvv37zcATEFBgTHmyP51uVxm6dKlkZgvv/zSADBr1661Ks1jfDtvY4y56KKLzP/8z/9Yl5RSu3btzJ///OeTZl8frSF3Y06e/S1ps9+A/H4/NmzYgJEjR0aus9vtGDlyJNauXWthZjrbtm1DZmYmcnJy8JOf/AQ7d+60OqUWKS4uRklJSdT+93q9yMvLOyn2/5o1a5CamoqePXvixhtvxMGDB61O6RgVFRUAgPbt2wMANmzYgEAgELXPe/XqhS5durSpff7tvBv89a9/RceOHdG3b1/MmjULtbXy/JnvSygUwosvvoiamhrk5+efNPsaODb3Bm15f2u1udWwG5SVlSEUCiEtLS3q+rS0NHz11VcWZaWTl5eHxYsXo2fPnti3bx/mzJmDCy+8EFu3bkViYqLV6amUlJQAQKP7v+G2tmrMmDGYMGECunXrhqKiIvzmN7/B2LFjsXbtWjgcuiGFJ1o4HMYtt9yC888/H3379gVwZJ+73W4kJydHxbalfd5Y3gAwadIkZGdnIzMzE5s3b8btt9+OwsJCLFu2zMJsgS1btiA/Px/19fVISEjA8uXL0bt3b2zatKnN7+umcgfa7v5uqTZbgE5mY8eOjfw7NzcXeXl5yM7OxksvvYTrr7/ewsxOD1dddVXk3/369UNubi66d++ONWvWYMSIERZm9h/Tp0/H1q1b2+TfBpvTVN7Tpk2L/Ltfv37IyMjAiBEjUFRUhO7du3/faUb07NkTmzZtQkVFBV5++WVMnjwZBQUFluXTEk3l3rt37za7v1uqzf4KrmPHjnA4HMeclVJaWor09HSLsjo+ycnJOPPMM7F9+3arU1Fr2Menwv7PyclBx44d28z+nzFjBt544w2sXr06avZVeno6/H4/ysvLo+Lbyj5vKu/G5OXlAYDl+9ztdqNHjx4YOHAg5s6di/79++Oxxx5r8/saaDr3xrSV/d1SbbYAud1uDBw4EKtWrYpcFw6HsWrVqqjfg54MqqurUVRUhIyMDKtTUevWrRvS09Oj9n9lZSU+/vjjk27/7969GwcPHrR8/xtjMGPGDCxfvhzvvfceunXrFnX7wIED4XK5ovZ5YWEhdu7caek+l/JuzKZNmwDA8n3+beFwGD6fr83u6+Y05N6Ytrq/RVafBdGcF1980Xg8HrN48WLzxRdfmGnTppnk5GRTUlJidWrN+uUvf2nWrFljiouLzYcffmhGjhxpOnbsaPbv3291alGqqqrMxo0bzcaNGw0AM2/ePLNx40azY8cOY4wxDz74oElOTjavvvqq2bx5sxk/frzp1q2bqaura7N5V1VVmV/96ldm7dq1pri42Lz77rvmnHPOMWeccYapr6+3NO8bb7zReL1es2bNGrNv377Ipba2NhJzww03mC5dupj33nvPrF+/3uTn55v8/HwLs5bz3r59u7n33nvN+vXrTXFxsXn11VdNTk6OGTp0qKV533HHHaagoMAUFxebzZs3mzvuuMPYbDbz9ttvG2Pa5r5u0FzubXV/H482XYCMMeaJJ54wXbp0MW6325x77rlm3bp1VqckuvLKK01GRoZxu92mU6dO5sorrzTbt2+3Oq1jrF692gA45jJ58mRjzJFTse+66y6TlpZmPB6PGTFihCksLLQ2adN83rW1tWbUqFEmJSXFuFwuk52dbaZOndomPrQ0ljMAs2jRokhMXV2duemmm0y7du1MXFycufzyy82+ffusS9rIee/cudMMHTrUtG/f3ng8HtOjRw9z2223mYqKCkvz/u///m+TnZ1t3G63SUlJMSNGjIgUH2Pa5r5u0FzubXV/Hw/OAyIiIku02b8BERHRqY0FiIiILMECRERElmABIiIiS7AAERGRJViAiIjIEixARERkCRYgIiKyBAsQERFZggWIiIgswQJERESW+P8J8ue3EAM9aQAAAABJRU5ErkJggg==\n"},"metadata":{}}],"source":["envs.reset()\n","\n","# number of agents\n","num_agents = envs.num_envs\n","print('Number of agents:', num_agents)\n","\n","init_screen = envs.get_screen().to(device)\n","_, _, screen_height, screen_width = init_screen.shape\n","\n","# size of each action\n","action_size = envs.action_space.shape[0]\n","print('Size of each action:', action_size)\n","\n","plt.figure()\n","plt.imshow(init_screen[0].cpu().squeeze(0).permute(1, 2, 0).numpy(),\n","           interpolation='none')\n","plt.title('Example extracted screen')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OvXdLRFCwtjf"},"outputs":[],"source":["def get_screen():\n","    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n","    # such as 800x1200x3. Transpose it into torch order (CHW).\n","    #env.render(mode='human')\n","    screen = env._get_observation().transpose((2, 0, 1))\n","    # Convert to float, rescale, convert to torch tensor\n","    # (this doesn't require a copy)\n","    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n","    screen = torch.from_numpy(screen)\n","    # Resize, and add a batch dimension (BCHW)\n","    return resize(screen).unsqueeze(0).to(device)\n","def eval_policy(envs, policy, tmax=1000):\n","    reward_list=[]\n","    state = envs.reset()\n","    for t in range(tmax):\n","        states = get_screen()\n","        action_est, values = policy(states)\n","        sigma = nn.Parameter(torch.zeros(action_size))\n","        dist = torch.distributions.Normal(action_est, F.softplus(sigma).to(device))\n","        actions = dist.sample()\n","        _, reward, done, _  = envs.step(actions[0])\n","        dones = done\n","        reward_list.append(np.mean(reward))\n","\n","        # stop if any of the trajectories is done to have retangular lists\n","        if np.any(dones):\n","            break\n","    return reward_list"]},{"cell_type":"markdown","metadata":{"id":"Pn9U0h_Owtjf"},"source":["## Network Architecture\n","An actor-critic structure with continuous action space is used for this project. The policy consists of 3 parts, a shared hidden layers, actor, and critic.\n","The actor layer outputs the mean value of a normal distribution, from which the agent's action is sampled. The critic layer yields the value function.\n","\n","- Shared layer:\n","```\n","Input State(48,48,3) -> Conv2d(3, 16, 5, 2) -> BatchNorm2d(16) -> Conv2d(16, 32, 5, 2)-> BatchNorm2d(32)\n","-> Conv2d(32, 32, 5, 2) -> BatchNorm2d(32) -> Dense(128) -> LeakyReLU -> Dense(128) -> LeakyReLU -> Dense(64) -> LeakyReLU\n","```\n","- Actor and Critic layers:\n","```\n","LeakyRelu -> Dense(64) -> LeakyRelu -> Dense(4)-> tanh -> Actor's output\n","LeakyReLU -> Dense(64) -> LeakyRelu -> Dense(1) -> Critic's output\n","```\n","\n","### Model update using PPO/GAE\n","The hyperparameters used during training are:\n","\n","Parameter | Value | Description\n","------------ | ------------- | -------------\n","Number of Agents | 1 | Number of agents trained simultaneously\n","tmax | 20 | Maximum number of steps per episode\n","Epochs | 10 | Number of training epoch per batch sampling\n","Batch size | 128 | Size of batch taken from the accumulated  trajectories\n","Discount (gamma) | 0.993 | Discount rate\n","Epsilon | 0.07 | Ratio used to clip r = new_probs/old_probs during training\n","Gradient clip | 10.0 | Maximum gradient norm\n","Beta | 0.01 | Entropy coefficient\n","Tau | 0.95 | tau coefficient in GAE\n","Learning rate | 2e-4 | Learning rate\n","Optimizer | Adam | Optimization method\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EbCSY2guwtjf"},"outputs":[],"source":["# Define hyperparameters\n","state_size = screen_height * screen_width * 3  # Assuming RGB images\n","action_size = envs.action_space.shape[0]\n","hidden_layers = [128, 64]\n","lr = 3e-4\n","gamma = 0.99\n","tau = 0.005\n","alpha = 0.2\n","batch_size = 256\n","buffer_size = 1000000\n","\n","# Initialize SAC policy\n","policy = SACActorCritic(\n","    state_size=state_size,\n","    action_size=action_size,\n","    hidden_layers=hidden_layers,\n","    init_type='xavier-uniform',\n","    seed=0\n",").to(device)\n","\n","# Initialize target entropy\n","policy.target_entropy = -torch.prod(torch.Tensor(envs.action_space.shape).to(device)).item()\n","\n","# Create separate optimizers for actor, critic, and alpha\n","actor_optimizer = optim.Adam(policy.actor.parameters(), lr=lr)\n","critic_optimizer = optim.Adam(list(policy.critic1.parameters()) + list(policy.critic2.parameters()), lr=lr)\n","alpha_optimizer = optim.Adam([policy.log_alpha], lr=lr)\n","\n","# Initialize replay buffer\n","memory = ReplayBuffer(buffer_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9-vvcMFwtjf"},"outputs":[],"source":["PATH = 'policy_sac.pt'"]},{"cell_type":"code","source":["# Hyperparameters (using previously defined values)\n","writer = SummaryWriter()\n","\n","total_steps = 0\n","episode_reward = 0\n","state = envs.reset()\n","save_scores = []\n","scores_window = deque(maxlen=100)  # last 100 scores\n","start_time = timeit.default_timer()\n","best_mean_reward = None\n","season = 1000000\n","tmax = 1000//num_agents   #env episode steps\n","\n","for s in range(season):\n","    for step in range(tmax):\n","        state_tensor = torch.FloatTensor(state).reshape(1, -1).to(device)  # Flatten the state\n","        action, _ = policy.sample(state_tensor)\n","        action = action.cpu().detach().numpy()[0]\n","\n","        next_state, reward, done, _ = envs.step(action)\n","        memory.push(state.flatten(), action, reward, next_state.flatten(), done)\n","\n","        state = next_state\n","        episode_reward += reward\n","        total_steps += 1\n","\n","        if len(memory) > batch_size:\n","            update_sac(policy, critic_optimizer, actor_optimizer, alpha_optimizer, memory, batch_size, gamma, tau, device)\n","\n","        if done:\n","            state = envs.reset()\n","            scores_window.append(episode_reward)\n","            save_scores.append(episode_reward)\n","            writer.add_scalar(\"Episode Reward\", episode_reward, total_steps)\n","            episode_reward = 0\n","\n","    # Evaluate after each season\n","    mean_reward = np.mean(scores_window)\n","    writer.add_scalar(\"Mean Reward (100 episodes)\", mean_reward, s)\n","\n","    if best_mean_reward is None or best_mean_reward < mean_reward:\n","        torch.save({\n","            'policy_state_dict': policy.state_dict(),\n","            'critic_optimizer_state_dict': critic_optimizer.state_dict(),\n","            'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n","            'alpha_optimizer_state_dict': alpha_optimizer.state_dict()\n","        }, PATH)\n","        if best_mean_reward is not None:\n","            print(f\"Best mean reward updated {best_mean_reward:.3f} -> {mean_reward:.3f}, model saved\")\n","        best_mean_reward = mean_reward\n","\n","    if mean_reward > 50:\n","        print(f'Environment solved in {s+1} seasons!\\tAverage Score: {mean_reward:.2f}')\n","        break\n","\n","print(f'Average Score: {mean_reward:.2f}')\n","elapsed = timeit.default_timer() - start_time\n","print(f\"Elapsed time: {timedelta(seconds=elapsed)}\")\n","writer.close()\n","envs.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"id":"m3TUhV0opg6M","executionInfo":{"status":"error","timestamp":1731272162849,"user_tz":300,"elapsed":2670,"user":{"displayName":"Kausar Patherya","userId":"09012208137279792936"}},"outputId":"b942942a-fc39-429f-d7ec-06f005d6eab8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-28-7cb274f04177>:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n","  state_tensor = torch.FloatTensor(state).reshape(1, -1).to(device)  # Flatten the state\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (1x165888 and 4800x128)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-7cb274f04177>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten the state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-9795e136e319>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mnormal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-9795e136e319>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mlog_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_std\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x165888 and 4800x128)"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_obbGY8Qwtjg"},"outputs":[],"source":["fig = plt.figure()\n","plt.plot(np.arange(len(save_scores)), save_scores)\n","plt.ylabel('Score')\n","plt.xlabel('Season #')\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","source":["def plot_training_progress(scores, epsilons, betas, losses, value_mses, episode_lengths):\n","    fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n","\n","    axs[0, 0].plot(scores)\n","    axs[0, 0].set_title('Score per Episode')\n","    axs[0, 0].set_xlabel('Episode')\n","    axs[0, 0].set_ylabel('Score')\n","\n","    axs[0, 1].plot(epsilons, label='Epsilon')\n","    axs[0, 1].plot(betas, label='Beta')\n","    axs[0, 1].set_title('Epsilon and Beta Decay')\n","    axs[0, 1].set_xlabel('Episode')\n","    axs[0, 1].legend()\n","\n","    axs[1, 0].plot(losses)\n","    axs[1, 0].set_title('Total Loss')\n","    axs[1, 0].set_xlabel('Episode')\n","    axs[1, 0].set_ylabel('Loss')\n","\n","    axs[1, 1].plot(value_mses)\n","    axs[1, 1].set_title('Value Function MSE')\n","    axs[1, 1].set_xlabel('Episode')\n","    axs[1, 1].set_ylabel('MSE')\n","\n","    axs[2, 0].plot(episode_lengths)\n","    axs[2, 0].set_title('Mean Episode Length')\n","    axs[2, 0].set_xlabel('Episode')\n","    axs[2, 0].set_ylabel('Length')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"ALDdndUfeL7b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qYT70E1Mwtjh"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VU2EKr4Rwtjh"},"outputs":[],"source":["episode = 10\n","scores_window = deque(maxlen=100)  # last 100 scores\n","env = KukaDiverseObjectEnv(renders=False, isDiscrete=False, removeHeightHack=False, maxSteps=20, isTest=True)\n","env.cid = p.connect(p.DIRECT)\n","# load the model\n","checkpoint = torch.load(PATH)\n","policy.load_state_dict(checkpoint['policy_state_dict'])\n","\n","# evaluate the model\n","for e in range(episode):\n","    rewards = eval_policy(envs=env, policy=policy)\n","    reward = np.sum(rewards,0)\n","    print(\"Episode: {0:d}, reward: {1}\".format(e+1, reward), end=\"\\n\")"]},{"cell_type":"code","source":["# After training, call the function with your collected data\n","plot_training_progress(save_scores, epsilons, betas, losses, value_mses, episode_lengths)"],"metadata":{"id":"-z6vzOZ9gA1g"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}